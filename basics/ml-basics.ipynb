{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Basics\n",
    "\n",
    "We are going to go over a few ML Basics to get the basic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# dense\n",
    "v1 = Vectors.dense(3,2,1)\n",
    "print(v1)\n",
    "\n",
    "# sparse\n",
    "v2 = Vectors.sparse(10, (0, 9), (100, 200))\n",
    "print(v2)\n",
    "print(v2.toArray())\n",
    "\n",
    "## TODO \n",
    "# declare a sparse vector that has lenght of 100\n",
    "# every 10 element filled with a (random) value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Training & Testing\n",
    "Run the following cell a few times, and observe the test / train sets.\n",
    "Each run will have differnet data for train/test.\n",
    "\n",
    "Q : How can we always get the same data for training and test?\n",
    "hint : Set the seed value to any integer   \n",
    "df.randomSplit (weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.range(1,100)\n",
    "df.show()\n",
    "(train, test) = df.randomSplit([0.7, 0.3])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", train.count())\n",
    "train.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()\n",
    "\n",
    "common = train.intersect(test)\n",
    "print(\"----common data set-----\")\n",
    "print(\"count: \", common.count())\n",
    "common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "(training, test) = dataset.randomSplit([0.8, 0.2])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", training.count())\n",
    "training.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assemblers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"gre\", \"gpa\", \"rank\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df) \n",
    "feature_vector.show(40) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## String Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"color\":['red', 'white', 'blue', 'blue', 'white' ,'yellow', 'blue' ]})\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert it to spark df\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run String Indexer\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "str_indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "\n",
    "model = str_indexer.fit(df_spark)\n",
    "indexed = model.transform(df_spark)\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import IndexToString\n",
    "\n",
    "converter = IndexToString(inputCol=\"colorIndex\", outputCol=\"originalColor\")\n",
    "converted = converter.transform(indexed)\n",
    "converted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df2_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"status\":['married', 'single', 'single', 'divorced', 'married' ,'single', 'married' ]})\n",
    "df2_pd\n",
    "df2_spark = spark.createDataFrame(df2_pd)\n",
    "df2_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "# first String Indexer\n",
    "string_indexer = StringIndexer(inputCol=\"status\", outputCol=\"statusIndex\")\n",
    "model = string_indexer.fit(df2_spark)\n",
    "indexed = model.transform(df2_spark)\n",
    "indexed.show()\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"statusIndex\", outputCol=\"statusVector\", dropLast=False)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n",
    "\n",
    "# View dense vectors in pandas\n",
    "encoded_pd = encoded.toPandas()\n",
    "print(encoded_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "\n",
    "df_pd = pd.DataFrame({\"home_runs\": [ 30,  22,  17,  12, 44,   38,  40], \n",
    "                      \"salary_in_k\":[ 700, 450,340, 250, 1200, 800, 950 ]})\n",
    "df_pd\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"home_runs\", \"salary_in_k\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df_spark) \n",
    "feature_vector.show(40) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(feature_vector)\n",
    "scaledData = scalerModel.transform(feature_vector)\n",
    "scaledData.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Basics\n",
    "\n",
    "We are going to go over a few ML Basics to get the basic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# dense\n",
    "v1 = Vectors.dense(3,2,1)\n",
    "print(v1)\n",
    "\n",
    "# sparse\n",
    "v2 = Vectors.sparse(10, (0, 9), (100, 200))\n",
    "print(v2)\n",
    "print(v2.toArray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Data\n",
    "Quick way to understand data set very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# use describe() on all columns\n",
    "df.describe().show()\n",
    "\n",
    "# use describe on one column : GRE\n",
    "df.describe('gre').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Training & Testing\n",
    "Run the following cell a few times, and observe the test / train sets.\n",
    "Each run will have differnet data for train/test.\n",
    "\n",
    "Q : How can we always get the same data for training and test?\n",
    "hint : Set the seed value to any integer   \n",
    "df.randomSplit (weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a range data\n",
    "df = spark.range(1,100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : let's split 70% for training and 30% for testing\n",
    "##    - first argument for randomSPlit is : 0.7  (representing 70%)\n",
    "##    - second argument for randomSPlit is : 0.3  (representing 30%)\n",
    "\n",
    "(train, test) = df.randomSplit([???, ???])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", train.count())\n",
    "train.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()\n",
    "\n",
    "## There should NO common data between training and test\n",
    "common = train.intersect(test)\n",
    "print(\"----common data set-----\")\n",
    "print(\"count: \", common.count())\n",
    "common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## now let's split a 'real world dataset'\n",
    "\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\",\\\n",
    "                         header=True, inferSchema=True)\n",
    "\n",
    "## TODO : split training 80%,  testing 20%\n",
    "## Hint : arguments are 0.8  and 0.2\n",
    "(training, test) = dataset.randomSplit([???, ???])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", training.count())\n",
    "training.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : evaluate how the data is split by 'admit' column\n",
    "## Hint : groupBy('admit')\n",
    "print(\"training data split\")\n",
    "training.groupBy(\"???\").count().show()\n",
    "\n",
    "print(\"testing data split\")\n",
    "test.groupBy(\"???\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assemblers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                    header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a vector consisting : gre, gpa , rank\n",
    "## we call this vector 'features'\n",
    "assembler = VectorAssembler(inputCols=[\"gre\", \"gpa\", \"rank\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df) \n",
    "feature_vector.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## String Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"color\":['red', 'white', 'blue', 'blue', 'white' ,'yellow', 'blue' ]})\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert it to spark df\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run String Indexer\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "str_indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "\n",
    "model = str_indexer.fit(df_spark)\n",
    "indexed = model.transform(df_spark)\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "converter = IndexToString(inputCol=\"colorIndex\", outputCol=\"originalColor\")\n",
    "converted = converter.transform(indexed)\n",
    "converted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 : create a pandas df and then a spark df\n",
    "import pandas as pd\n",
    "\n",
    "df2_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"status\":['married', 'single', 'single', 'divorced', 'married' ,'single', 'married' ]})\n",
    "df2_pd\n",
    "df2_spark = spark.createDataFrame(df2_pd)\n",
    "df2_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 2 : convert  categorical data to indexes \n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "# first String Indexer\n",
    "string_indexer = StringIndexer(inputCol=\"status\", outputCol=\"statusIndex\")\n",
    "model = string_indexer.fit(df2_spark)\n",
    "indexed = model.transform(df2_spark)\n",
    "indexed.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 3 : encode the indexes into a vector\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"statusIndex\", outputCol=\"statusVector\", dropLast=False)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n",
    "\n",
    "# View dense vectors in pandas\n",
    "encoded_pd = encoded.toPandas()\n",
    "print(encoded_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Scaling Data\n",
    "\n",
    "### StandardScaler\n",
    "[Standard Scaler documentation](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: create a pandas df and then spark df\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "\n",
    "df_pd = pd.DataFrame({\"home_runs\": [ 30,  22,  17,  12, 44,   38,  40], \n",
    "                      \"salary_in_k\":[ 700, 450,340, 250, 1200, 800, 950 ]})\n",
    "df_pd\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 2 : create a vector\n",
    "assembler = VectorAssembler(inputCols=[\"home_runs\", \"salary_in_k\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df_spark) \n",
    "feature_vector.show(40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 3 : Scale data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(feature_vector)\n",
    "scaledData = scalerModel.transform(feature_vector)\n",
    "scaledData.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "[MinMaxScaler docs](https://spark.apache.org/docs/2.1.0/ml-features.html#minmaxscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 4 : Try a MinMaxScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "## TODO : define minMaxScaler with  min=1  and max=100\n",
    "mmScaler = MinMaxScaler(min=???, max=???, inputCol=\"features\", outputCol=\"scaled_features2\")\n",
    "scaledModel2 = mmScaler.fit(feature_vector)\n",
    "scaledData2 = scaledModel2.transform(feature_vector)\n",
    "scaledData2.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Basics\n",
    "\n",
    "We are going to go over a few ML Basics to get the basic concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n",
      "Spark found in :  /Users/sujee/spark\n",
      "Spark config:\n",
      "\t spark.app.name=TestApp\n",
      "\tspark.master=local[*]\n",
      "\texecutor.memory=2g\n",
      "\tspark.sql.warehouse.dir=/var/folders/lp/qm_skljd2hl4xtps5vw0tdgm0000gn/T/tmp9s92_c44\n",
      "\tsome_property=some_value\n",
      "Spark UI running on port 4040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TestApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1047a10f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0,2.0,1.0]\n",
      "(10,[0,9],[100.0,200.0])\n",
      "[100.   0.   0.   0.   0.   0.   0.   0.   0. 200.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# dense\n",
    "v1 = Vectors.dense(3,2,1)\n",
    "print(v1)\n",
    "\n",
    "# sparse\n",
    "v2 = Vectors.sparse(10, (0, 9), (100, 200))\n",
    "print(v2)\n",
    "print(v2.toArray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Describe Data\n",
    "Quick way to understand data set very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+\n",
      "|admit|gre| gpa|rank|\n",
      "+-----+---+----+----+\n",
      "|    0|380|3.61|   3|\n",
      "|    1|660|3.67|   3|\n",
      "|    1|800| 4.0|   1|\n",
      "|    0|640|3.19|   4|\n",
      "|    0|520|2.93|   4|\n",
      "|    1|760| 3.0|   2|\n",
      "|    0|560|2.98|   1|\n",
      "|    0|400|3.08|   2|\n",
      "|    0|540|3.39|   3|\n",
      "|    1|700|3.92|   2|\n",
      "|    1|800| 4.0|   4|\n",
      "|    0|440|3.22|   1|\n",
      "|    1|760| 4.0|   1|\n",
      "|    1|700|3.08|   2|\n",
      "|    1|700| 4.0|   1|\n",
      "|    0|480|3.44|   3|\n",
      "|    1|780|3.87|   4|\n",
      "|    0|360|2.56|   3|\n",
      "|    1|800|3.75|   2|\n",
      "|    0|540|3.81|   1|\n",
      "+-----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------------------+------------------+------------------+-----------------+\n",
      "|summary|              admit|               gre|               gpa|             rank|\n",
      "+-------+-------------------+------------------+------------------+-----------------+\n",
      "|  count|                100|               100|               100|              100|\n",
      "|   mean|               0.43|             600.0| 3.390699999999998|             2.52|\n",
      "| stddev|0.49756985195624304|124.46248065545332|0.3971877275408833|1.019803902718557|\n",
      "|    min|                  0|               300|              2.42|                1|\n",
      "|    max|                  1|               800|               4.0|                4|\n",
      "+-------+-------------------+------------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               gre|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|             600.0|\n",
      "| stddev|124.46248065545332|\n",
      "|    min|               300|\n",
      "|    max|               800|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# use describe() on all columns\n",
    "df.describe().show()\n",
    "\n",
    "# use describe on one column : GRE\n",
    "df.describe('gre').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.43</td>\n",
       "      <td>600.0</td>\n",
       "      <td>3.390699999999998</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.49756985195624304</td>\n",
       "      <td>124.46248065545332</td>\n",
       "      <td>0.3971877275408833</td>\n",
       "      <td>1.019803902718557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                admit                 gre                 gpa  \\\n",
       "0   count                  100                 100                 100   \n",
       "1    mean                 0.43               600.0   3.390699999999998   \n",
       "2  stddev  0.49756985195624304  124.46248065545332  0.3971877275408833   \n",
       "3     min                    0                 300                2.42   \n",
       "4     max                    1                 800                 4.0   \n",
       "\n",
       "                rank  \n",
       "0                100  \n",
       "1               2.52  \n",
       "2  1.019803902718557  \n",
       "3                  1  \n",
       "4                  4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pretty describe using Pandas\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Split Dataset into Training & Testing\n",
    "Run the following cell a few times, and observe the test / train sets.\n",
    "Each run will have differnet data for train/test.\n",
    "\n",
    "Q : How can we always get the same data for training and test?\n",
    "hint : Set the seed value to any integer   \n",
    "df.randomSplit (weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create a range data\n",
    "df = spark.range(1,100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----training data set-----\n",
      "count:  67\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 16|\n",
      "| 17|\n",
      "| 19|\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 25|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "----testing data set-----\n",
      "count:  32\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  9|\n",
      "| 15|\n",
      "| 18|\n",
      "| 24|\n",
      "| 31|\n",
      "| 32|\n",
      "| 34|\n",
      "| 36|\n",
      "| 37|\n",
      "| 38|\n",
      "| 39|\n",
      "| 41|\n",
      "| 46|\n",
      "| 49|\n",
      "| 50|\n",
      "| 55|\n",
      "| 61|\n",
      "| 63|\n",
      "| 65|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "----common data set-----\n",
      "count:  0\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO : let's split 70% for training and 30% for testing\n",
    "##    - first argument for randomSPlit is : 0.7  (representing 70%)\n",
    "##    - second argument for randomSPlit is : 0.3  (representing 30%)\n",
    "\n",
    "(train, test) = df.randomSplit([0.7, 0.3])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", train.count())\n",
    "train.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()\n",
    "\n",
    "## There should NO common data between training and test\n",
    "common = train.intersect(test)\n",
    "print(\"----common data set-----\")\n",
    "print(\"count: \", common.count())\n",
    "common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----training data set-----\n",
      "count:  82\n",
      "+-----+---+----+----+\n",
      "|admit|gre| gpa|rank|\n",
      "+-----+---+----+----+\n",
      "|    0|300|2.92|   4|\n",
      "|    0|360|2.56|   3|\n",
      "|    0|360|3.14|   1|\n",
      "|    0|380|2.91|   4|\n",
      "|    0|380|2.94|   3|\n",
      "|    0|380|3.61|   3|\n",
      "|    0|400|3.05|   2|\n",
      "|    0|400|3.31|   3|\n",
      "|    0|400|3.35|   3|\n",
      "|    0|400|3.65|   2|\n",
      "|    0|440|2.48|   4|\n",
      "|    0|440|3.13|   4|\n",
      "|    0|480|3.39|   4|\n",
      "|    0|480|3.57|   2|\n",
      "|    0|500|2.71|   2|\n",
      "|    0|500|2.97|   4|\n",
      "|    0|500|3.17|   3|\n",
      "|    0|520|2.93|   4|\n",
      "|    0|520|2.98|   2|\n",
      "|    0|520|3.29|   1|\n",
      "+-----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "----testing data set-----\n",
      "count:  18\n",
      "+-----+---+----+----+\n",
      "|admit|gre| gpa|rank|\n",
      "+-----+---+----+----+\n",
      "|    0|400|3.08|   2|\n",
      "|    0|440|3.22|   1|\n",
      "|    0|480|3.44|   3|\n",
      "|    0|500|3.31|   3|\n",
      "|    0|520| 2.9|   3|\n",
      "|    0|560|2.98|   1|\n",
      "|    0|560|3.36|   3|\n",
      "|    0|580|3.69|   1|\n",
      "|    0|600|3.48|   2|\n",
      "|    0|640|3.52|   4|\n",
      "|    0|640| 4.0|   3|\n",
      "|    1|460|3.45|   3|\n",
      "|    1|620| 4.0|   1|\n",
      "|    1|660|3.63|   2|\n",
      "|    1|680|3.85|   3|\n",
      "|    1|720|3.64|   1|\n",
      "|    1|760|3.35|   2|\n",
      "|    1|800|3.73|   1|\n",
      "+-----+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## now let's split a 'real world dataset'\n",
    "\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\",\\\n",
    "                         header=True, inferSchema=True)\n",
    "\n",
    "## TODO : split training 80%,  testing 20%\n",
    "## Hint : arguments are 0.8  and 0.2\n",
    "(training, test) = dataset.randomSplit([0.8, 0.2])\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", training.count())\n",
    "training.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", test.count())\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data split\n",
      "+-----+-----+\n",
      "|admit|count|\n",
      "+-----+-----+\n",
      "|    1|   36|\n",
      "|    0|   46|\n",
      "+-----+-----+\n",
      "\n",
      "testing data split\n",
      "+-----+-----+\n",
      "|admit|count|\n",
      "+-----+-----+\n",
      "|    1|    7|\n",
      "|    0|   11|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO : evaluate how the data is split by 'admit' column\n",
    "## Hint : groupBy('admit')\n",
    "print(\"training data split\")\n",
    "training.groupBy(\"admit\").count().show()\n",
    "\n",
    "print(\"testing data split\")\n",
    "test.groupBy(\"admit\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Vector Assemblers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+\n",
      "|admit|gre| gpa|rank|\n",
      "+-----+---+----+----+\n",
      "|    0|380|3.61|   3|\n",
      "|    1|660|3.67|   3|\n",
      "|    1|800| 4.0|   1|\n",
      "|    0|640|3.19|   4|\n",
      "|    0|520|2.93|   4|\n",
      "|    1|760| 3.0|   2|\n",
      "|    0|560|2.98|   1|\n",
      "|    0|400|3.08|   2|\n",
      "|    0|540|3.39|   3|\n",
      "|    1|700|3.92|   2|\n",
      "|    1|800| 4.0|   4|\n",
      "|    0|440|3.22|   1|\n",
      "|    1|760| 4.0|   1|\n",
      "|    1|700|3.08|   2|\n",
      "|    1|700| 4.0|   1|\n",
      "|    0|480|3.44|   3|\n",
      "|    1|780|3.87|   4|\n",
      "|    0|360|2.56|   3|\n",
      "|    1|800|3.75|   2|\n",
      "|    0|540|3.81|   1|\n",
      "+-----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                    header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+----------------+\n",
      "|admit|gre| gpa|rank|        features|\n",
      "+-----+---+----+----+----------------+\n",
      "|    0|380|3.61|   3|[380.0,3.61,3.0]|\n",
      "|    1|660|3.67|   3|[660.0,3.67,3.0]|\n",
      "|    1|800| 4.0|   1| [800.0,4.0,1.0]|\n",
      "|    0|640|3.19|   4|[640.0,3.19,4.0]|\n",
      "|    0|520|2.93|   4|[520.0,2.93,4.0]|\n",
      "|    1|760| 3.0|   2| [760.0,3.0,2.0]|\n",
      "|    0|560|2.98|   1|[560.0,2.98,1.0]|\n",
      "|    0|400|3.08|   2|[400.0,3.08,2.0]|\n",
      "|    0|540|3.39|   3|[540.0,3.39,3.0]|\n",
      "|    1|700|3.92|   2|[700.0,3.92,2.0]|\n",
      "|    1|800| 4.0|   4| [800.0,4.0,4.0]|\n",
      "|    0|440|3.22|   1|[440.0,3.22,1.0]|\n",
      "|    1|760| 4.0|   1| [760.0,4.0,1.0]|\n",
      "|    1|700|3.08|   2|[700.0,3.08,2.0]|\n",
      "|    1|700| 4.0|   1| [700.0,4.0,1.0]|\n",
      "|    0|480|3.44|   3|[480.0,3.44,3.0]|\n",
      "|    1|780|3.87|   4|[780.0,3.87,4.0]|\n",
      "|    0|360|2.56|   3|[360.0,2.56,3.0]|\n",
      "|    1|800|3.75|   2|[800.0,3.75,2.0]|\n",
      "|    0|540|3.81|   1|[540.0,3.81,1.0]|\n",
      "|    0|500|3.17|   3|[500.0,3.17,3.0]|\n",
      "|    1|660|3.63|   2|[660.0,3.63,2.0]|\n",
      "|    0|600|2.82|   4|[600.0,2.82,4.0]|\n",
      "|    0|680|3.19|   4|[680.0,3.19,4.0]|\n",
      "|    1|760|3.35|   2|[760.0,3.35,2.0]|\n",
      "|    1|800|3.66|   1|[800.0,3.66,1.0]|\n",
      "|    1|620|3.61|   1|[620.0,3.61,1.0]|\n",
      "|    1|520|3.74|   4|[520.0,3.74,4.0]|\n",
      "|    1|780|3.22|   2|[780.0,3.22,2.0]|\n",
      "|    0|520|3.29|   1|[520.0,3.29,1.0]|\n",
      "|    0|540|3.78|   4|[540.0,3.78,4.0]|\n",
      "|    1|760|3.35|   3|[760.0,3.35,3.0]|\n",
      "|    1|600| 3.4|   3| [600.0,3.4,3.0]|\n",
      "|    1|800| 4.0|   3| [800.0,4.0,3.0]|\n",
      "|    0|360|3.14|   1|[360.0,3.14,1.0]|\n",
      "|    0|400|3.05|   2|[400.0,3.05,2.0]|\n",
      "|    0|580|3.25|   1|[580.0,3.25,1.0]|\n",
      "|    0|520| 2.9|   3| [520.0,2.9,3.0]|\n",
      "|    1|500|3.13|   2|[500.0,3.13,2.0]|\n",
      "|    1|520|2.68|   3|[520.0,2.68,3.0]|\n",
      "+-----+---+----+----+----------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create a vector consisting : gre, gpa , rank\n",
    "## we call this vector 'features'\n",
    "assembler = VectorAssembler(inputCols=[\"gre\", \"gpa\", \"rank\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df) \n",
    "feature_vector.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: String Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   color\n",
       "0   1     red\n",
       "1   2   white\n",
       "2   3    blue\n",
       "3   4    blue\n",
       "4   5   white\n",
       "5   6  yellow\n",
       "6   7    blue"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"color\":['red', 'white', 'blue', 'blue', 'white' ,'yellow', 'blue' ]})\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| color|\n",
      "+---+------+\n",
      "|  1|   red|\n",
      "|  2| white|\n",
      "|  3|  blue|\n",
      "|  4|  blue|\n",
      "|  5| white|\n",
      "|  6|yellow|\n",
      "|  7|  blue|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert it to spark df\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id| color|colorIndex|\n",
      "+---+------+----------+\n",
      "|  1|   red|       2.0|\n",
      "|  2| white|       1.0|\n",
      "|  3|  blue|       0.0|\n",
      "|  4|  blue|       0.0|\n",
      "|  5| white|       1.0|\n",
      "|  6|yellow|       3.0|\n",
      "|  7|  blue|       0.0|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run String Indexer\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "str_indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "\n",
    "model = str_indexer.fit(df_spark)\n",
    "indexed = model.transform(df_spark)\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Reverse String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-------------+\n",
      "| id| color|colorIndex|originalColor|\n",
      "+---+------+----------+-------------+\n",
      "|  1|   red|       2.0|          red|\n",
      "|  2| white|       1.0|        white|\n",
      "|  3|  blue|       0.0|         blue|\n",
      "|  4|  blue|       0.0|         blue|\n",
      "|  5| white|       1.0|        white|\n",
      "|  6|yellow|       3.0|       yellow|\n",
      "|  7|  blue|       0.0|         blue|\n",
      "+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "converter = IndexToString(inputCol=\"colorIndex\", outputCol=\"originalColor\")\n",
    "converted = converter.transform(indexed)\n",
    "converted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|  status|\n",
      "+---+--------+\n",
      "|  1| married|\n",
      "|  2|  single|\n",
      "|  3|  single|\n",
      "|  4|divorced|\n",
      "|  5| married|\n",
      "|  6|  single|\n",
      "|  7| married|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : create a pandas df and then a spark df\n",
    "import pandas as pd\n",
    "\n",
    "df2_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"status\":['married', 'single', 'single', 'divorced', 'married' ,'single', 'married' ]})\n",
    "df2_pd\n",
    "df2_spark = spark.createDataFrame(df2_pd)\n",
    "df2_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "| id|  status|statusIndex|\n",
      "+---+--------+-----------+\n",
      "|  1| married|        1.0|\n",
      "|  2|  single|        0.0|\n",
      "|  3|  single|        0.0|\n",
      "|  4|divorced|        2.0|\n",
      "|  5| married|        1.0|\n",
      "|  6|  single|        0.0|\n",
      "|  7| married|        1.0|\n",
      "+---+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 2 : convert  categorical data to indexes \n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "# first String Indexer\n",
    "string_indexer = StringIndexer(inputCol=\"status\", outputCol=\"statusIndex\")\n",
    "model = string_indexer.fit(df2_spark)\n",
    "indexed = model.transform(df2_spark)\n",
    "indexed.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-------------+\n",
      "| id|  status|statusIndex| statusVector|\n",
      "+---+--------+-----------+-------------+\n",
      "|  1| married|        1.0|(3,[1],[1.0])|\n",
      "|  2|  single|        0.0|(3,[0],[1.0])|\n",
      "|  3|  single|        0.0|(3,[0],[1.0])|\n",
      "|  4|divorced|        2.0|(3,[2],[1.0])|\n",
      "|  5| married|        1.0|(3,[1],[1.0])|\n",
      "|  6|  single|        0.0|(3,[0],[1.0])|\n",
      "|  7| married|        1.0|(3,[1],[1.0])|\n",
      "+---+--------+-----------+-------------+\n",
      "\n",
      "   id    status  statusIndex     statusVector\n",
      "0   1   married          1.0  (0.0, 1.0, 0.0)\n",
      "1   2    single          0.0  (1.0, 0.0, 0.0)\n",
      "2   3    single          0.0  (1.0, 0.0, 0.0)\n",
      "3   4  divorced          2.0  (0.0, 0.0, 1.0)\n",
      "4   5   married          1.0  (0.0, 1.0, 0.0)\n",
      "5   6    single          0.0  (1.0, 0.0, 0.0)\n",
      "6   7   married          1.0  (0.0, 1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "## Step 3 : encode the indexes into a vector\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"statusIndex\", outputCol=\"statusVector\", dropLast=False)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n",
    "\n",
    "# View dense vectors in pandas\n",
    "encoded_pd = encoded.toPandas()\n",
    "print(encoded_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8:  Scaling Data\n",
    "\n",
    "### 8.1: StandardScaler\n",
    "[Standard Scaler documentation](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|home_runs|salary_in_k|\n",
      "+---------+-----------+\n",
      "|       30|        700|\n",
      "|       22|        450|\n",
      "|       17|        340|\n",
      "|       12|        250|\n",
      "|       44|       1200|\n",
      "|       38|        800|\n",
      "|       40|        950|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: create a pandas df and then spark df\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "\n",
    "df_pd = pd.DataFrame({\"home_runs\": [ 30,  22,  17,  12, 44,   38,  40], \n",
    "                      \"salary_in_k\":[ 700, 450,340, 250, 1200, 800, 950 ]})\n",
    "df_pd\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|home_runs|salary_in_k|     features|\n",
      "+---------+-----------+-------------+\n",
      "|       30|        700| [30.0,700.0]|\n",
      "|       22|        450| [22.0,450.0]|\n",
      "|       17|        340| [17.0,340.0]|\n",
      "|       12|        250| [12.0,250.0]|\n",
      "|       44|       1200|[44.0,1200.0]|\n",
      "|       38|        800| [38.0,800.0]|\n",
      "|       40|        950| [40.0,950.0]|\n",
      "+---------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 2 : create a vector\n",
    "assembler = VectorAssembler(inputCols=[\"home_runs\", \"salary_in_k\"], outputCol=\"features\") \n",
    "feature_vector = assembler.transform(df_spark) \n",
    "feature_vector.show(40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+---------------------------------------+\n",
      "|home_runs|salary_in_k|features     |scaled_features                        |\n",
      "+---------+-----------+-------------+---------------------------------------+\n",
      "|30       |700        |[30.0,700.0] |[2.4359938288234506,2.03376119068933]  |\n",
      "|22       |450        |[22.0,450.0] |[1.7863954744705304,1.3074179083002835]|\n",
      "|17       |340        |[17.0,340.0] |[1.3803965029999552,0.987826864049103] |\n",
      "|12       |250        |[12.0,250.0] |[0.9743975315293802,0.7263432823890463]|\n",
      "|44       |1200       |[44.0,1200.0]|[3.5727909489410608,3.4864477554674225]|\n",
      "|38       |800        |[38.0,800.0] |[3.0855921831763706,2.324298503644948] |\n",
      "|40       |950        |[40.0,950.0] |[3.2479917717646005,2.760104473078376] |\n",
      "+---------+-----------+-------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 3 : Scale data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(feature_vector)\n",
    "scaledData = scalerModel.transform(feature_vector)\n",
    "scaledData.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2: MinMaxScaler\n",
    "[MinMaxScaler docs](https://spark.apache.org/docs/2.1.0/ml-features.html#minmaxscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-----------------------------+\n",
      "|home_runs|salary_in_k|features     |scaled_features2             |\n",
      "+---------+-----------+-------------+-----------------------------+\n",
      "|30       |700        |[30.0,700.0] |[56.6875,47.89473684210526]  |\n",
      "|22       |450        |[22.0,450.0] |[31.9375,21.842105263157894] |\n",
      "|17       |340        |[17.0,340.0] |[16.46875,10.378947368421054]|\n",
      "|12       |250        |[12.0,250.0] |[1.0,1.0]                    |\n",
      "|44       |1200       |[44.0,1200.0]|[100.0,100.0]                |\n",
      "|38       |800        |[38.0,800.0] |[81.4375,58.31578947368421]  |\n",
      "|40       |950        |[40.0,950.0] |[87.625,73.94736842105263]   |\n",
      "+---------+-----------+-------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 4 : Try a MinMaxScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "## TODO : define minMaxScaler with  min=1  and max=100\n",
    "mmScaler = MinMaxScaler(min=1, max=100, inputCol=\"features\", outputCol=\"scaled_features2\")\n",
    "scaledModel2 = mmScaler.fit(feature_vector)\n",
    "scaledData2 = scaledModel2.transform(feature_vector)\n",
    "scaledData2.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

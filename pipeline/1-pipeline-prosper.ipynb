{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIpeline: Prosper Loan Dataset\n",
    "\n",
    "Pipelines are very useful for combining many steps together.\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/prosper-loan/prosper-loan-data.csv.gz\", \n",
    "                         header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our columns for convenience.\n",
    "\n",
    "columns = ['Term', 'BorrowerRate', 'ProsperRating (numeric)', 'ProsperScore', 'EmploymentStatusDuration', 'IsBorrowerHomeowner',\n",
    "           'CreditScore', 'CurrentCreditLines', 'OpenCreditLines',\n",
    "           'TotalCreditLinespast7years', 'OpenRevolvingAccounts', 'OpenRevolvingMonthlyPayment',\n",
    "           'InquiriesLast6Months', 'TotalInquiries', 'CurrentDelinquencies', 'AmountDelinquent',\n",
    "           'DelinquenciesLast7Years', 'PublicRecordsLast10Years', 'PublicRecordsLast12Months',\n",
    "           'RevolvingCreditBalance', 'BankcardUtilization', 'AvailableBankcardCredit', 'TotalTrades',\n",
    "           'TradesNeverDelinquent (percentage)', 'TradesOpenedLast6Months', 'DebtToIncomeRatio',\n",
    "           'IncomeVerifiable', 'StatedMonthlyIncome', 'TotalProsperLoans', 'TotalProsperPaymentsBilled',\n",
    "           'OnTimeProsperPayments', 'ProsperPaymentsLessThanOneMonthLate', 'ProsperPaymentsOneMonthPlusLate',\n",
    "           'ProsperPrincipalBorrowed', 'ProsperPrincipalOutstanding', 'LoanOriginalAmount',\n",
    "           'MonthlyLoanPayment', 'Recommendations', 'InvestmentFromFriendsCount', 'InvestmentFromFriendsAmount',\n",
    "           'Investors', 'YearsWithCredit']\n",
    "\n",
    "categorical_columns = [\"BorrowerState\", \"EmploymentStatus\", \"ListingCategory\"]\n",
    "categorical_indexers = [\"BorrowerState_index\", \"EmploymentStatus_index\", \"ListingCategory_index\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.select(columns).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: drop all NAs\n",
    "\n",
    "Go ahead and drop all NAs.\n",
    "\n",
    "**=> TODO: Drop all NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = # TODO Drop NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Examine the contents of the categorical columns.\n",
    "\n",
    "Let's look at the contents of our categorical columns.\n",
    "\n",
    "**=> TODO: Group by categorical columns LoanStatus BorrowerState, EmploymentStatus, ListingCategory\n",
    "**and see the breakdowns by count** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.groupBy(???).count().show()\n",
    "dataset.groupBy(???).count().show()\n",
    "dataset.groupBy(???).count().show()\n",
    "dataset.groupBy(???).count().show(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does that say about the cardinality of these categorical columns? ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Converting Categorical columns \n",
    "\n",
    "We need to convert categorical columns to numerics.  Remember, remember, Spark ML can *only* handle numeric columns.  There's a tool called StringIndexer that will help us here.\n",
    "\n",
    "Because there are a lot of indexers, we build a pipeline to help us out here. \n",
    "\n",
    "**=> TODO: enter in the list with all of our indexers into the pipeline**\n",
    "HINT: take each column, and then output the column + \"_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(categorical_columns)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=???, outputCol=???, handleInvalid=\"keep\").\\\n",
    "            fit(dataset) for column in categorical_columns ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build feature vectors using VectorAssembler.\n",
    "\n",
    "**=> TODO: enter input cols as columns + categorical_indexers, outputCol = features**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=???, outputCol=???) #TODO: create vector assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build Indexers\n",
    "\n",
    "We are going to load the label indexer, which will make us a label column for loan status.\n",
    "We will also add feature indexer which will identify which features are categorical. (we should have 3.)\n",
    "\n",
    "**=>TODO: Which column is your output label?**\n",
    "**=>TODO: Enter input column for label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"???\", outputCol=\"indexedLabel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the data.  We will use standardscaler for this. This will normalize all data.\n",
    "\n",
    "**=> TODO: instantiate standardscaler with inputcol=indexedFeatures, outputCol=scaledFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"???\", outputCol=\"???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now).\n",
    "\n",
    "**=> TODO: Split dataset into 70% training, 30% validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run the pipeline that will fit our decision tree\n",
    "\n",
    "We have a 8 stage pipeline here: \n",
    "\n",
    " 1. CategoryIndexer1\n",
    " 2. CategoryIndexer2\n",
    " 3. CategoryIndexer3\n",
    " 4. VectorAssembler \n",
    " 5. LabelIndexer\n",
    " 6. FeatureIndexer\n",
    " 7. Scaler\n",
    " 8. RandomForestClassifier\n",
    " \n",
    "Running the pipeline will do all eight.  Note that our other indexer pipeline already ran above.\n",
    "\n",
    "**=> TODO: Add indexers list plus assembler, labelIndexer, featureIndexer, scaler, and rf to our pipeline**\n",
    " HINT: You should have 8 separate items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a DecisionTree model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"scaledFeatures\", maxBins=10000, numTrees=20)\n",
    "\n",
    "stages = indexers + [???] # TODO enter the six stages to the pipeline\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions\n",
    "\n",
    "**=> TODO: make predicitons on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(???) #Make predictions on test data.\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"scaledFeatures\").show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

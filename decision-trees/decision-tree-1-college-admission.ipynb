{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning: College Admission\n",
    "\n",
    "A decision tree a learned set of rules that allows us to make decisions on data.\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \n",
    "                         header=True, inferSchema=True)\n",
    "## clean \n",
    "dataset = dataset.na.drop()\n",
    "\n",
    "dataset.printSchema()\n",
    "dataset.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Look at data split by 'admit' colunm\n",
    "Looks like a resaonble distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : group by 'admit' column\n",
    "dataset.groupBy('???').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Summary of data\n",
    "use 'describe' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build feature vectors using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : create a feature vector using the following attributes\n",
    "## Hint :   'gpa' ,   'gre',   'rank'\n",
    "assembler = VectorAssembler(inputCols=['???', '???', '???'], outputCol=\"features\")\n",
    "feature_vector = assembler.transform(dataset)\n",
    "\n",
    "## TODO : set 'admit' as 'label' column\n",
    "feature_vector = feature_vector.withColumn(\"label\", feature_vector[\"???\"])\n",
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now).\n",
    "\n",
    "**=> TODO: Split dataset into 80% training, 20% validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Split the data into 80% training and 20% test sets \n",
    "## Hint :  0.8,   0.2\n",
    "(training, test) =  feature_vector.randomSplit([???,   ???])\n",
    "#(training, test) =  feature_vector.randomSplit([???,   ???], seed=123)  # seed can be any number\n",
    "print(\"training set = \" , training.count())\n",
    "print(\"testing set = \" , test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "## TODO : Create a DecisionTree model with maximum bins = 500\n",
    "## \n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=???)\n",
    "\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO : train model on training dataset\n",
    "## Hint : pass 'training' dataset name\n",
    "\n",
    "print(\"training starting...\")\n",
    "model = dt.fit(???)\n",
    "print(\"training done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Print Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## print the model\n",
    "print(model)\n",
    "print()\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Get Predictions on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : Get predictions on test data\n",
    "## Hint : 'test' set name\n",
    "predictions_test = model.transform(???)\n",
    "predictions_test.show()\n",
    "\n",
    "## do a sample label\n",
    "predictions_test.sampleBy(col=\"label\", fractions={0: 0.5, 1: 0.5}, seed=123).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.transform(???) # Hint : test\n",
    "predictions_train = model.transform(???)  # Hint : training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "print(\"Training set accuracy = \" , evaluator.evaluate(predictions_train))\n",
    "print(\"Test set accuracy = \" , evaluator.evaluate(predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Group by 'admit' \n",
    "cm = predictions_test.groupBy('admit').pivot('prediction', [0,1]).count().na.fill(0).orderBy('admit')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "cm_pd.set_index(\"admit\", inplace=True)\n",
    "# print(cm_pd)\n",
    "\n",
    "# colormaps : cmap=\"YlGnBu\" , cmap=\"Greens\", cmap=\"Blues\",  cmap=\"Reds\"\n",
    "sns.heatmap(cm_pd, annot=True, fmt=',', cmap=\"Blues\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Do a Few Runs\n",
    "- Click 'Cell --> Run All'\n",
    "- Observe how the 'accuracy' output changes above\n",
    "- Why?  Can we get the same accuracy all the times?\n",
    "\n",
    "To get 'consistant' split for testing and random data you can set the seed variable\n",
    "\n",
    "```python\n",
    "(training, test) =  feature_vector.randomSplit([0.7,   0.3], seed=123)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

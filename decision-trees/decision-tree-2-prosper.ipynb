{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning: Prosper Loan Dataset\n",
    "\n",
    "A decision tree a learned set of rules that allows us to make decisions on data.\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## small file, start with this\n",
    "datafile = \"/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "\n",
    "## this is a large file\n",
    "# datafile = \"/data/prosper-loan/prosper-loan-data.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = spark.read. \\\n",
    "          option(\"header\", \"true\"). \\\n",
    "          option(\"inferSchema\", \"true\").  \\\n",
    "          csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read {:,} records\".format(data.count()))\n",
    "# schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print with pandas\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : select a few columns \n",
    "## start with: 'LoanStatus', 'ProsperScore',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome'\n",
    "## we can add more later\n",
    "\n",
    "select_columns = ['LoanStatus', 'ProsperScore', '???', '???', '???', '???']\n",
    "\n",
    "## Note : vector columns can only have Numbers, don't include Categorical columns here\n",
    "## And dfefinitely not 'LoanStatus'  (if you are curiuos include and see what happens!)\n",
    "vector_columns = ['ProsperScore', 'EmpIndex', 'CreditScore', 'StatedMonthlyIncome']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : Extract only the columns we are interested in\n",
    "## Hint : 'select_columns'\n",
    "prosper = data.select(???)  \n",
    "\n",
    "print (prosper.count())\n",
    "prosper.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Drop any NA, null values.  \n",
    "## Hint : Using `.na.drop()`\n",
    "prosper_clean = prosper.na.???()\n",
    "\n",
    "print(\"Original record count {:,}, cleaned records count {:,},  dropped {:,}\"\\\n",
    "      .format(prosper.count(), prosper_clean.count(), (prosper.count() - prosper_clean.count())))\n",
    "prosper_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some summary data\n",
    "\n",
    "**=> Q : What does that say about the cardinality of these categorical columns?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : use 'describe()' and then 'toPandas()'\n",
    "prosper_clean.???().???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Look at some summaries\n",
    "## We are going to group counts by 'LoanStatus',  'EmploymentStatus'\n",
    "\n",
    "prosper_clean.groupBy('LoanStatus').count().show()\n",
    "prosper_clean.groupBy('EmploymentStatus').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Converting Categorical columns \n",
    "\n",
    "Convert categorical columns to numeric.   \n",
    "Here let's convert **EmploymentStatus** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "## TODO : Create a StringIndexer with inputCol='EmploymentStatus'  and outputCol = 'EmpIndex'\n",
    "strIndexer_employment = StringIndexer(inputCol=\"???\", outputCol=\"???\")\n",
    "prosper_indexed = strIndexer_employment.fit(prosper_clean).transform(prosper_clean)\n",
    "\n",
    "prosper_indexed.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build feature vectors using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=vector_columns, outputCol=\"features\")\n",
    "feature_vector = assembler.transform(prosper_indexed)\n",
    "feature_vector = feature_vector.withColumn(\"label\", feature_vector[\"LoanStatus\"])\n",
    "\n",
    "feature_vector.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO :  Split the data into 70% training and 30% test sets \n",
    "## Hint : 0.7   , 0.3\n",
    "(training, test) =  feature_vector.randomSplit([???,  ???])\n",
    "print(\"training set = \" , training.count())\n",
    "print(\"testing set = \" , test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "##  TODO: Create a DecisionTree model with 5000 Maxbins\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Train Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO : train with training data set\n",
    "## Hint : training\n",
    "\n",
    "print (\"training starting...\")\n",
    "tree_model = dt.fit(???)\n",
    "print (\"training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Print Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : Observe the output\n",
    "## how many nodes does the tree have?\n",
    "print(tree_model)\n",
    "print()\n",
    "print(tree_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : create predictions using test dataset\n",
    "## Hint : test\n",
    "predictions = tree_model.transform(???)\n",
    "\n",
    "predictions2= predictions.drop('rawPrediction', 'probability')\n",
    "predictions2.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy \",  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('LoanStatus').pivot('prediction', [0,1]).count().na.fill(0).orderBy('LoanStatus').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Improve Accuracy\n",
    "\n",
    "### Add more data\n",
    "In Step-1 change the 'datafile' to the full dataset.  \n",
    "And see how the accuracy above changes\n",
    "\n",
    "### Add more features\n",
    "Look at the schema of the full dataset.  Are there any columns you want to add"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

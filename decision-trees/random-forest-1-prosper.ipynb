{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest : Prosper Loan Dataset\n",
    "\n",
    "A decision tree a learned set of rules that allows us to make decisions on data.\n",
    "\n",
    "We are going to look at the prosper loan dataset.  This dataset shows a history of loans made by Prosper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## small file, start with this\n",
    "datafile = \"/data/prosper-loan/prosper-loan-data-sample.csv\"\n",
    "\n",
    "## this is a large file\n",
    "#datafile = \"/data/prosper-loan/prosper-loan-data.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read. \\\n",
    "          option(\"header\", \"true\"). \\\n",
    "          option(\"inferSchema\", \"true\").  \\\n",
    "          csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read {:,} records\".format(data.count()))\n",
    "# schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print with pandas\n",
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## select a few columns \n",
    "## start with: 'LoanStatus',  'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome'\n",
    "## we add more later\n",
    "\n",
    "select_columns = ['LoanStatus',  'ProsperScore', 'EmploymentStatus', 'CreditScore', 'StatedMonthlyIncome', 'ListingCategory']\n",
    "\n",
    "## Note : vector columns can only have Numbers, don't include Categorical columns here\n",
    "## And dfefinitely not 'LoanStatus'  (if you are curiuos include and see what happens!)\n",
    "vector_columns = [  'ProsperScore', 'EmpIndex', 'CreditScore', 'StatedMonthlyIncome']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Display\n",
    "\n",
    "prosper = data.select(select_columns)  \n",
    "prosper.printSchema()\n",
    "\n",
    "prosper.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop any NA values.  Using `dataframe.na.drop()`\n",
    "prosper_clean = prosper.na.drop()\n",
    "print(\"Original record count {:,}, cleaned records count {:,},  dropped {:,}\"\\\n",
    "      .format(prosper.count(), prosper_clean.count(), (prosper.count() - prosper_clean.count())))\n",
    "prosper_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prosper_clean.groupBy('LoanStatus').count().show()\n",
    "prosper_clean.groupBy('EmploymentStatus').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does that say about the cardinality of these categorical columns? ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Converting Categorical columns \n",
    "\n",
    "Convert categorical columns to numeric.   \n",
    "Here let's convert **EmploymentStatus** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "strIndexer_employment = StringIndexer(inputCol=\"EmploymentStatus\", outputCol=\"EmpIndex\")\n",
    "prosper_indexed = strIndexer_employment.fit(prosper_clean).transform(prosper_clean)\n",
    "\n",
    "prosper_indexed.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build feature vectors using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=vector_columns, outputCol=\"features\")\n",
    "feature_vector = assembler.transform(prosper_indexed)\n",
    "feature_vector = feature_vector.withColumn(\"label\", feature_vector[\"LoanStatus\"])\n",
    "\n",
    "feature_vector.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data into training and test.\n",
    "\n",
    "We will split our the data up into training and test.  (You know the drill by now).\n",
    "\n",
    "**=> TODO: Split dataset into 70% training, 30% validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training, test) =  feature_vector.randomSplit([???,???])\n",
    "print(\"training set = \" , training.count())\n",
    "print(\"testing set = \" , test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "## TODO : Create a RandomForest with numTrees=20  and maxBins=10000\n",
    "\n",
    "rf = ???(labelCol=\"label\", featuresCol=\"features\", \\\n",
    "                            numTrees=???, maxBins=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.2 Train the RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"training starting...\")\n",
    "## TODO : train in training data\n",
    "rf_model = rf.fit(???)\n",
    "print (\"training done.\")\n",
    "\n",
    "## TODO : NOtice the time it took for training\n",
    "## Is it more or less than decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"traind on {:,} records using {:,} features in {:,.2f} ms\".\\\n",
    "      format(training.count(), len(vector_columns), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Print RF\n",
    "**==>Q : How many nodes the tree has?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## print the model\n",
    "print(rf_model)\n",
    "print()\n",
    "print(rf_model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO : predict on test data\n",
    "predictions = rf_model.transform(???)\n",
    "\n",
    "predictions2= predictions.drop('rawPrediction', 'probability')\n",
    "predictions2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the model.\n",
    "\n",
    "Let us check to see how the model did, using accuracy as a measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy \",  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('LoanStatus').pivot('prediction', [0,1]).count().na.fill(0).orderBy('LoanStatus').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Improve Accuracy\n",
    "\n",
    "### Add more data\n",
    "In Step-1 change the 'datafile' to the full dataset.  \n",
    "And see how the accuracy above changes\n",
    "\n",
    "### Add more features\n",
    "Look at the schema of the full dataset.  Are there any columns you want to add"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests: Presidential Contributions\n",
    "\n",
    "Let's look at a random forests models for the presidential dataset.\n",
    "\n",
    "This dataset defines all presidential contribution amounts from publicly available information.\n",
    "\n",
    "The purpose here is to try to classify the candidate to whom the contributor contributes.  \n",
    "\n",
    "Here are the feature columns we will use:\n",
    "1. Last Name (converted from Contributor Name)\n",
    "2. First Name (converted from Contributor Name)\n",
    "3. State \n",
    "4. Latitude (converted from Zipcode)\n",
    "5. Longitude (converted from zipcode)\n",
    "6. Employer\n",
    "7. Occupation\n",
    "\n",
    "### Notes\n",
    "\n",
    "This is going to be a very difficult dataset to get high accuracy, because we don't have any features that are highly correlated with the outcome. Part of our analysis is to see which features prove to be the most useful. \n",
    "\n",
    "One might suspect that information like State, might be very predictive -- because presumably New Yorkers might contribute to Hillary Clinton and Texans might contribute to Donald Trump. However, it turns out that State is pretty weakly correlated to the outcome.  \n",
    "\n",
    "One nice thing about random forests is that since we \"bag\" featues in differnet trees, we can empirically see which variables have hte most predictive power.  This is helpful for analytical reasons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "dataset = spark.read.csv(\"/data/presidential_election_contribs/2016/2016-medium-clean.csv\", \\\n",
    "                         header=True, inferSchema=True)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(dataset.count(), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = ['CAND_NM']\n",
    "numeric_columns = ['LAT', 'LNG']\n",
    "feature_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'LAT', 'LNG', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_index = ['FIRSTNAME_index', 'LASTNAME_index', 'CONTBR_ST_index', 'CONTBR_EMPLOYER_index', \n",
    "                     \"CONTBR_OCCUPATION_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out a contribution count broken down by candidate?\n",
    "**=> Q : Which candidates got the most donations? (in terms of number of donors) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : print out per candidate breakdown\n",
    "## Hint : What column represents Candidate name\n",
    "dataset.groupBy('???').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : sort the output by number of contributions\n",
    "dataset.groupBy('???').count().orderBy('???', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Indexers and feature vector\n",
    "\n",
    "Let's index all the categorical columns, and build a labeld index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\").fit(dataset) for column in categorical_columns ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r2 = pipeline.fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler2 = VectorAssembler(inputCols=numeric_columns + categorical_index, outputCol=\"features\")\n",
    "fv2 = assembler2.transform(df_r2.na.drop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"CAND_NM\", outputCol=\"indexedLabel\").fit(fv2)\n",
    "\n",
    "\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer2 =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(fv2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Craete Random Forest Model with Pipeline\n",
    "\n",
    "Create the model here and add it to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "## TODO : Create a RandomForest Model with  numTrees=20 and  maxBins=10000\n",
    "rf2 = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=???, maxBins=???)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline2 = Pipeline(stages=[labelIndexer, featureIndexer2, rf2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split data into training and test\n",
    "\n",
    "**=> TODO: build training and test datasets 70%/30% **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : split 70% training and 30% testing\n",
    "## Hint : 0.7 ,  0.3\n",
    "(trainingData2, testData2) = fv2.randomSplit([???,  ???]\n",
    "\n",
    "print(\"training set = \" , trainingData2.count())\n",
    "print(\"testing set = \" , testData2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting model training....this will take some time\")\n",
    "t1 = time.perf_counter()\n",
    "## TODO : train the model with our training set\n",
    "## Hint : trainingData2\n",
    "model2 = pipeline2.fit(???)\n",
    "t2 = time.perf_counter()\n",
    "print(\"trained on {:,} records  in {:,.2f} ms\".\\\n",
    "      format(trainingData2.count(),  (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : predict with our test data\n",
    "## Hint : testData2\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "predictions2 = model2.transform(???)\n",
    "t2 = time.perf_counter()\n",
    "print(\"prediction on {:,} records  in {:,.2f} ms\".\\\n",
    "      format(testData2.count(),  (t2-t1)*1000))\n",
    "\n",
    "predictions2.select(feature_columns + ['probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaulate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions2.select(\"prediction\", \"CAND_NM\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator2.evaluate(predictions2)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel2 = model2.stages[2]\n",
    "print(rfModel2)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Q: Think about the test error here?  Does it seem high?  What does that say about our model?**\n",
    "\n",
    "**=> Q: How do we define model success?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Label\n",
    "\n",
    "0. Hillary Clinton\n",
    "1. Bernie Sanders\n",
    "2. Donald Trump\n",
    "3. Ted Cruz\n",
    "\n",
    "## Step 7: Print out the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2.groupBy('CAND_NM').pivot('prediction', range(0,22)).count().na.fill(0).orderBy('CAND_NM').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the list above to interpret the label.  \n",
    "\n",
    "**=>What can you conclude from the confusion matrix?**\n",
    "\n",
    "Is our model better at predicting candidates with many donations (Clinton, Sanders), or few donations?\n",
    "\n",
    "What can you say about our model perfromance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print the feature importanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imp = rfModel2.featureImportances.toArray()\n",
    "print(imp)\n",
    "cols = numeric_columns + categorical_columns\n",
    "print(cols)\n",
    "df = pd.DataFrame({'cols': cols, 'importance':imp})\n",
    "print(df)\n",
    "df.sort_values(by=['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Most important Fields\n",
    "\n",
    "1. Employer\n",
    "2. Occupation\n",
    "3. LastName\n",
    "4. State\n",
    "\n",
    "Other fields not significant\n",
    "\n",
    "**=> TODO Compare the relative weight of the feature importances?**\n",
    "\n",
    "Why do you think that the lat/long and other fields did not contribute?\n",
    "\n",
    "**=> BONUS: Do a Pearson Correlation Matrix of the variables to the outcome, to see correlation|**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS : Running on full dataset\n",
    "\n",
    "**Use the dowload script**\n",
    "\n",
    "```bash\n",
    "$ cd   ~/data/presidential_election_contribs\n",
    "$ ./download-data.sh\n",
    "```\n",
    "\n",
    "This will download full dataset.\n",
    "\n",
    "As we run on larger dataset, the execution will take longer and Jupyter notebook might time out.  So let's run this in command line / script mode\n",
    "\n",
    "Download the Jupyter notebook as Python file (File --> Download as --> Python)\n",
    "\n",
    "```bash\n",
    "# run the downloaded python script as follows\n",
    "$    time  ~/spark/bin/spark-submit    --master local[*]  random-forest-2-election-classification.py 2> logs\n",
    "\n",
    "```\n",
    "\n",
    "Watch the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

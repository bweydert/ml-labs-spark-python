{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests: Presidential Contributions\n",
    "\n",
    "Let's look at a random forests models for the presidential dataset.\n",
    "\n",
    "This dataset defines all presidential contribution amounts from publicly available information.\n",
    "\n",
    "The purpose here is to try to classify the candidate to whom the contributor contributes.  \n",
    "\n",
    "Here are the feature columns we will use:\n",
    "1. Last Name (converted from Contributor Name)\n",
    "2. First Name (converted from Contributor Name)\n",
    "3. State \n",
    "4. Latitude (converted from Zipcode)\n",
    "5. Longitude (converted from zipcode)\n",
    "6. Employer\n",
    "7. Occupation\n",
    "\n",
    "### Notes\n",
    "\n",
    "This is going to be a very difficult dataset to get high accuracy, because we don't have any features that are highly correlated with the outcome. Part of our analysis is to see which features prove to be the most useful. \n",
    "\n",
    "One might suspect that information like State, might be very predictive -- because presumably New Yorkers might contribute to Hillary Clinton and Texans might contribute to Donald Trump. However, it turns out that State is pretty weakly correlated to the outcome.  \n",
    "\n",
    "One nice thing about random forests is that since we \"bag\" featues in differnet trees, we can empirically see which variables have hte most predictive power.  This is helpful for analytical reasons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import isnan, when, count, col, split, trim, countDistinct, abs \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import pyspark.sql.functions\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/presidential_election_contribs/2016/2016-medium-clean.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_column = ['CAND_NM']\n",
    "numeric_columns = ['LAT', 'LNG']\n",
    "feature_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'LAT', 'LNG', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_index = ['FIRSTNAME_index', 'LASTNAME_index', 'CONTBR_ST_index', 'CONTBR_EMPLOYER_index', \n",
    "                     \"CONTBR_OCCUPATION_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Print out a count broken down by candidate? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out a grouping by candidate\n",
    "# TODO: Print Breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Which candidates got the most donations? (in terms of number of donors) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Indexers and feature vector\n",
    "\n",
    "Let's index all the categorical columns, and build a labeld index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\").fit(dataset) for column in categorical_columns ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r2 = pipeline.fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Build vectors from all the numeric and categorical_index columns **\n",
    "**=> TODO: Make a new column called \"label\" which is the candidate name **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler2 = VectorAssembler(inputCols=??? + ???, outputCol=\"features\")\n",
    "fv2 = assembler2.transform(df_r2.na.drop())\n",
    "\n",
    "fv2 = fv2.withColumn(\"???\",???)   # TODO Create index from candidate name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(fv2)\n",
    "\n",
    "\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer2 =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(fv2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Random Forest Model with Pipeline\n",
    "\n",
    "Train the model here.\n",
    "\n",
    "**=> TODO: Create pipeline with labelIndexer, featureIndexer2, rf2 **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a RandomForest model.\n",
    "rf2 = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=20, maxBins=10000)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline2 = Pipeline(stages=[???, ???, ???])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split data into training and test\n",
    "\n",
    "**=> TODO: build training and test datasets 70%/30% **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData2, testData2) = ??? # do a random split 70%/30%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "\n",
    "**=> TODO: Get predictions by transforming testData2 **\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "\n",
    "model2 = pipeline2.fit(trainingData2)\n",
    "\n",
    "# Make predictions.\n",
    "predictions2 = model2.transform(???)  #Transform test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions2.select(feature_columns + ['probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaulate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select example rows to display.\n",
    "predictions2.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator2.evaluate(predictions2)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel2 = model2.stages[2]\n",
    "print(rfModel2)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Label\n",
    "\n",
    "0. Hillary Clinton\n",
    "1. Bernie Sanders\n",
    "2. Donald Trump\n",
    "3. Ted Cruz\n",
    "\n",
    "## Step 7: Print out the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions2.groupBy('label').pivot('prediction', range(0,22)).count().na.fill(0).orderBy('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the list above to interpret the label.  \n",
    "\n",
    "**=>What can you conclude from the confusion matrix?**\n",
    "\n",
    "Is our model better at predicting candidates with many donations (Clinton, Sanders), or few donations?\n",
    "\n",
    "What can you say about our model perfromance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print the feature importanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfModel2.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(numeric_columns + categorical_columns_donation) # for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Most important Fields\n",
    "\n",
    "1. Employer\n",
    "2. Occupation\n",
    "3. LastName\n",
    "4. State\n",
    "\n",
    "Other fields not significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n",
    "Why do you think that the lat/long and other fields did not contribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> BONUS: Do a Pearson Correlation Matrix of the variables to the outcome, to see correlation **\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests: Presidential Contributions\n",
    "\n",
    "Let's look at a random forests models for the presidential dataset.\n",
    "\n",
    "This dataset defines all presidential contribution amounts from publicly available information.\n",
    "\n",
    "**The purpose here is to try to classify the candidate to whom the contributor likely to contribute.**  \n",
    "\n",
    "Here are the feature columns we will use:\n",
    "1. State \n",
    "2. Employer\n",
    "3. Occupation\n",
    "\n",
    "### Notes\n",
    "\n",
    "This is going to be a very difficult dataset to get high accuracy, because we don't have any features that are highly correlated with the outcome. Part of our analysis is to see which features prove to be the most useful. \n",
    "\n",
    "One might suspect that information like State, might be very predictive -- because presumably New Yorkers might contribute to Hillary Clinton and Texans might contribute to Donald Trump. However, it turns out that State is pretty weakly correlated to the outcome.  \n",
    "\n",
    "One nice thing about random forests is that since we \"bag\" featues in differnet trees, we can empirically see which variables have hte most predictive power.  This is helpful for analytical reasons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 100k samples\n",
    "data_file = '/data/presidential_election_contribs/2016/2016-100k.csv.gz'\n",
    "\n",
    "\n",
    "data = spark.read.csv(data_file, \\\n",
    "                         header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read {:,} records\".format(data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data.show() is hard to read\n",
    "## use Pandas to pretty print\n",
    "\n",
    "## vertical\n",
    "## TODO : 'toPandas'\n",
    "data.limit(3).???().T\n",
    "\n",
    "# horizontal\n",
    "# data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Sample Data\n",
    "Start with a small sample of data. Once the algorithm is working procss the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : set sample rate, start with 0.1\n",
    "# sample size :  10% --> 0.1,   100%  -> 1.0\n",
    "sample_size = ???\n",
    "\n",
    "data = data.sample(withReplacement=False, fraction=sample_size)\n",
    "print(\"sample size {:,} records\".format(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - extract only a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Select these columns \n",
    "## Hint ; 'CAND_NM', 'CONTBR_ST', 'CONTBR_EMPLOYER', 'CONTBR_OCCUPATION', 'CONTB_RECEIPT_AMT'\n",
    "\n",
    "columns = ['???', '???']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.select(columns)\n",
    "data2.printSchema()\n",
    "\n",
    "data2.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Clean data (drop null values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : drop any null values\n",
    "## Hint : na\n",
    "data_clean = data2.???.???()\n",
    "\n",
    "print(\"original data size = {:,}\".format(data2.count()) )\n",
    "print(\"clean data size = {:,}\".format(data_clean.count()) )\n",
    "print(\"droped records = {:,}\".format(data2.count() - data_clean.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Print out a contribution count broken down by candidate?\n",
    "\n",
    "**=> Which candidates got the most donations? (in terms of number of donors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : print out per candidate breakdown\n",
    "## Hint : group by 'CAND_NM'  and order by 'count'\n",
    "data_clean.groupBy('???').count().orderBy('???', ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - find min/max/average contribution per candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max,mean\n",
    "\n",
    "## TODO : what colum represents contribution amount?\n",
    "data_clean.groupBy('CAND_NM').\\\n",
    "        agg(min('???'), mean('???'), max('???')).\\\n",
    "        orderBy('CAND_NM').\\\n",
    "        show(40, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Whoah!  Negative Contributions!\n",
    "We see some negative contributions!   \n",
    "\n",
    "**Q==> Can you guys figure out why?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Filter out only positive contribs\n",
    "## Hint : use fileter(condition)\n",
    "## Hint : condition :   \n",
    "pos_contribs = data_clean.???(\"??? > 0\")\n",
    "\n",
    "print(\"original data size = {:,}\".format(data_clean.count()) )\n",
    "print(\"positive contributions data size = {:,}\".format(pos_contribs.count()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - now find min/max/median in positive contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max,mean\n",
    "\n",
    "print (\"sorted by CAND_NM\")\n",
    "\n",
    "pos_contribs.groupBy('CAND_NM').\\\n",
    "        agg(min('CONTB_RECEIPT_AMT'), mean('CONTB_RECEIPT_AMT'), max('CONTB_RECEIPT_AMT')).\\\n",
    "        orderBy('CAND_NM').\\\n",
    "        show(40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max,mean\n",
    "\n",
    "print(\"sorted by AVG contribution\")\n",
    "\n",
    "pos_contribs.groupBy('CAND_NM').\\\n",
    "        agg(min('CONTB_RECEIPT_AMT'), mean('CONTB_RECEIPT_AMT'), max('CONTB_RECEIPT_AMT')).\\\n",
    "        orderBy('avg(CONTB_RECEIPT_AMT)', ascending=False).\\\n",
    "        show(40, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 -- Find total contribution amount per candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max,mean\n",
    "\n",
    "print(\"sorted by total contribution\")\n",
    "\n",
    "pos_contribs.groupBy('CAND_NM').\\\n",
    "        sum('CONTB_RECEIPT_AMT').\\\n",
    "        orderBy('sum(CONTB_RECEIPT_AMT)', ascending=False).\\\n",
    "        show(40, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "## TODO build indexers for following categorical columns\n",
    "## CAND_NM,   CONTBR_ST,  CONTBR_EMPLOYER,  CONTBR_OCCUPATION\n",
    "\n",
    "indexer1 = StringIndexer(inputCol='CAND_NM', outputCol = \"CAND_NM_index\", handleInvalid=\"keep\")\n",
    "indexer2 = StringIndexer(inputCol='???', outputCol = \"???_index\", handleInvalid=\"keep\")\n",
    "indexer3 = StringIndexer(inputCol='???', outputCol = \"???_index\", handleInvalid=\"keep\")\n",
    "indexer4 = StringIndexer(inputCol='???', outputCol = \"???_index\", handleInvalid=\"keep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stash indexers into \n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## TODO : add all indexers into stages\n",
    "pipeline = Pipeline(stages=[indexer1, ???, ???, ???])\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TODO : fit and transform 'pos_contribs'  through pipeline\n",
    "indexed_data = pipeline.fit(???).transform(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data.printSchema()\n",
    "# indexed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Understand indexed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "indexed_data.groupBy(['CONTBR_ST', 'CONTBR_ST_index']).count()\\\n",
    "            .orderBy('CONTBR_ST_index', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employer\n",
    "indexed_data.groupBy(['CONTBR_EMPLOYER', 'CONTBR_EMPLOYER_index']).count()\\\n",
    "            .orderBy('CONTBR_EMPLOYER_index', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occupation\n",
    "indexed_data.groupBy(['CONTBR_OCCUPATION', 'CONTBR_OCCUPATION_index']).count()\\\n",
    "            .orderBy('CONTBR_OCCUPATION_index', ascending=False).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 -  Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## Create a feature vector using 'index' columns\n",
    "feature_columns = ['CONTBR_ST_index', '???_index', '???_index' ]\n",
    "\n",
    "assembler = VectorAssembler(inputCols= feature_columns,  outputCol=\"features\")\n",
    "feature_vector = assembler.transform(indexed_data)\n",
    "feature_vector.printSchema()\n",
    "\n",
    "feature_vector.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split data into training and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Split the data into training and test sets (30% held out for testing)\n",
    "(training, test) = feature_vector.randomSplit([??? , ???])\n",
    "\n",
    "print(\"training set = \" , training.count())\n",
    "print(\"testing set = \" , test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "## TODO : Create a random forest model\n",
    "##        what is the 'labelCol' ?\n",
    "rf = ???(labelCol=\"???\", featuresCol=\"features\", numTrees=20, maxBins=50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"training starting...\")\n",
    "\n",
    "## TODO : start training, using 'fit' method  on training data\n",
    "model = rf.???(???)\n",
    "print(\"training done\")\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trained on {:,} records\".format(training.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO : predict on 'test' columns\n",
    "##        use 'transform' method\n",
    "predictions = model.???(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predicted on {:,} records\".format(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.sample(False, 0.1).select(\"prediction\", 'CAND_NM_index', \"CAND_NM\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Acuracy\n",
    "**=> TODO: Think about the test error here?  Does it seem high?  What does that say about our model?**\n",
    "\n",
    "**=> How do we define model success?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"CAND_NM_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Figure Out Candidate Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Candidate Mapping\n",
    "candidate_mapping = indexed_data.groupBy(['CAND_NM', 'CAND_NM_index']).count()\n",
    "# candidate_mapping.orderBy('CAND_NM').show()\n",
    "candidate_mapping.orderBy('CAND_NM_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "**=>What can you conclude from the confusion matrix?**\n",
    "\n",
    "Use the list above to interpret the label.  \n",
    "\n",
    "Is our model better at predicting candidates with many donations (Clinton, Sanders), or few donations?\n",
    "\n",
    "What can you say about our model perfromance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.groupBy('CAND_NM').pivot('prediction', range(0,22)).count().na.fill(0).orderBy('CAND_NM').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 -  Print the feature importanes\n",
    "\n",
    "**=> TODO Compare the relative weight of the feature importances?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imp = model.featureImportances.toArray()\n",
    "print(imp)\n",
    "df = pd.DataFrame({'cols': feature_columns, 'importance':imp})\n",
    "print(df)\n",
    "df.sort_values(by=['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Most important Fields\n",
    "\n",
    "1. Employer\n",
    "2. Occupation\n",
    "3. State\n",
    "\n",
    "Other fields not significant\n",
    "\n",
    "**=> TODO Compare the relative weight of the feature importances?**\n",
    "\n",
    "**=> BONUS: Do a Pearson Correlation Matrix of the variables to the outcome, to see correlation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS : Running on full dataset\n",
    "\n",
    "**Use the dowload script**\n",
    "\n",
    "```bash\n",
    "$ cd   ~/data/presidential_election_contribs\n",
    "$ ./download-data.sh\n",
    "```\n",
    "\n",
    "This will download full dataset.\n",
    "\n",
    "As we run on larger dataset, the execution will take longer and Jupyter notebook might time out.  So let's run this in command line / script mode\n",
    "\n",
    "Download the Jupyter notebook as Python file (File --> Download as --> Python)\n",
    "\n",
    "```bash\n",
    "# run the downloaded python script as follows\n",
    "$    time  ~/spark/bin/spark-submit    --master local[*]  random-forest-2-election-classification.py 2> logs\n",
    "\n",
    "```\n",
    "\n",
    "Watch the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

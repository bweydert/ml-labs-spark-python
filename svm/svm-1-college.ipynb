{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines: College Admission\n",
    "\n",
    "Let's look at a classification example in Spark MLLib.  We looked at the college admission before. We can look again at this dataset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "dataset.printSchema()\n",
    "dataset.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use describe\n",
    "dataset.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data spread\n",
    "dataset.groupBy(\"admit\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : input cols : gre, gpa, rank\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['???', '???', '???'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "featureVector.sample(False, 0.1, seed=10).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split into training and test\n",
    "## TODO: create training and test with an 80/20 split\n",
    "(training, test) = featureVector.randomSplit([???, ???])\n",
    "\n",
    "print (\"training set count \", training.count())\n",
    "print (\"testing set count \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "## TODO : set MaxIter to 10\n",
    "lsvc = LinearSVC(maxIter=???, regParam=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the model\n",
    "## TODO : fit on 'training' set\n",
    "print (\"training starting...\")\n",
    "lsvcModel = lsvc.fit(???)\n",
    "print(\"training done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linearsSVC\n",
    "print (\"inputs : gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Iterations\n",
    "\n",
    "If any coefficient is zero, that variable won't be a factor in the decision !  \n",
    "Set **maxIter=50**  and try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the test set and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: transform the test data\n",
    "## HINT : test\n",
    "predictions_test = lsvcModel.transform(???)\n",
    "predictions_test.show()\n",
    "\n",
    "## sample\n",
    "predictions_test.sampleBy(\"label\", fractions={0: 0.5, 1: 0.5}, seed=0).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: See the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = lsvcModel.transform(???)  # Hint : test\n",
    "predictions_train = lsvcModel.transform(???)  # Hint : training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Accuracy\n",
    "\n",
    "**TODO: Compare 'training' & 'test' set accuracies**  \n",
    "Can you detect any overfitting / underfitting?\n",
    "\n",
    "\n",
    "**TODO: Increase 'maxIterations' and try again**  \n",
    "Does the accuracy go up?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "print(\"Training accuracy :\",  evaluator.evaluate(predictions_train))\n",
    "print(\"Test accuracy :\",  evaluator.evaluate(predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7.2 - Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = predictions_test.groupBy('label').pivot('prediction', [0,1]).count().na.fill(0).orderBy('label')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "cm_pd.set_index(\"label\", inplace=True)\n",
    "# print(cm_pd)\n",
    "\n",
    "# colormaps : cmap=\"YlGnBu\" , cmap=\"Greens\", cmap=\"Blues\",  cmap=\"Reds\"\n",
    "sns.heatmap(cm_pd, annot=True, cmap=\"Blues\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 - AUC\n",
    "**=> What does AUC mean?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# default metrics for BinaryClassificationEvaluator is 'areaUnderCurve'\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "# print (\"default metrics : \" ,evaluator.getMetricName())\n",
    "\n",
    "print(\"AUC for training: \" , evaluator.evaluate(predictions_train))\n",
    "print (\"AUC for test : \" , evaluator.evaluate(predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Try running a prediction on your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "newdata = pd.DataFrame({'gre' : [600, 700, 800], \n",
    "                        'gpa' : [4.0, 3.5, 3.2],\n",
    "                        'rank': [1,   2,   3]}\n",
    "             )\n",
    "print(newdata)\n",
    "\n",
    "## TODO : create a spark dataframe\n",
    "## Hint : input is 'newdata'\n",
    "spark_newdata = spark.createDataFrame(???)\n",
    "\n",
    "## TODO : create feature vector\n",
    "## Hint : spark_newdata\n",
    "newfeatures = assembler.transform(???)\n",
    "\n",
    "lsvcModel.transform(newfeatures).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Understanding the impact of Scaling Data\n",
    "Just now we have fed our input vector without scaling to SVM.  \n",
    "IN this section we are going to scale the data and see if it improves the prediction.  \n",
    "We will condense the code to focus on important stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 : Raw data (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "\n",
    "print (\"===== run with raw data (not scaled) =======\")\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                         header=True, inferSchema=True)\n",
    "assembler = VectorAssembler(inputCols=[ 'gre', 'gpa', 'rank'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "(training, test) = featureVector.randomSplit([0.8, 0.2], seed=123)\n",
    "lsvc = LinearSVC(maxIter=100, regParam=0.3, featuresCol='features')\n",
    "lsvcModel = lsvc.fit(training)\n",
    "print (\"inputs :  gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "\n",
    "predictions_train = lsvcModel.transform(training)\n",
    "predictions_test = lsvcModel.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print (\"Training AUC : \" , evaluator.evaluate(predictions_train))\n",
    "print (\"Test AUC : \" , evaluator.evaluate(predictions_test))  #AUC\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print(\"Training accuracy\",  evaluator.evaluate(predictions_train))\n",
    "print(\"Testing accuracy\",  evaluator.evaluate(predictions_test))\n",
    "print (\"===== END run with raw data (not scaled) =======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 : Scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "print (\"===== run with scaled data  =======\")\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                         header=True, inferSchema=True)\n",
    "assembler = VectorAssembler(inputCols=[ 'gre', 'gpa', 'rank'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"featuresScaled\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(featureVector)\n",
    "fv_scaled = scalerModel.transform(featureVector)\n",
    "fv_scaled.show(20, truncate=False)\n",
    "\n",
    "(training, test) = fv_scaled.randomSplit([0.8, 0.2], seed=123)  ## CHANGED\n",
    "lsvc = LinearSVC(maxIter=100, regParam=0.3, featuresCol='featuresScaled')  ## CHANGED\n",
    "\n",
    "lsvcModel = lsvc.fit(training)\n",
    "print (\"inputs :  gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "\n",
    "predictions_train = lsvcModel.transform(training)\n",
    "predictions_test = lsvcModel.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print (\"Training AUC : \" , evaluator.evaluate(predictions_train))\n",
    "print (\"Test AUC : \" , evaluator.evaluate(predictions_test))  #AUC\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "print(\"Training accuracy\",  evaluator.evaluate(predictions_train))\n",
    "print(\"Testing accuracy\",  evaluator.evaluate(predictions_test))\n",
    "\n",
    "print (\"===== END run with scaled data =======\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Discuss the findings\n",
    "\n",
    "### Coefficients\n",
    "Here are the coefficients from one sample run\n",
    "\n",
    "raw data run : \n",
    "inputs :  gre, gpa, rank\n",
    "Coefficients: [0.00730924862823,0.803788881405,-0.182571791707]\n",
    "Intercept: -7.016411699283878\n",
    "\n",
    "scaled data run:\n",
    "inputs :  gre, gpa, rank\n",
    "Coefficients: [0.985025239033,0.311565356517,-0.180265498388]\n",
    "Intercept: -7.500693768967119\n",
    "\n",
    "### Accuracy\n",
    "Compare accuracies"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

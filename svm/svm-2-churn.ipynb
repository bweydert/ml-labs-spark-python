{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines: Churn Analysis\n",
    "\n",
    "Let's look at a classification example in Spark MLLib.  We are going to look at some telecom data to see whether or not a customer \"churned\" or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "dataset = spark.read.csv(\"/data/churn/telco.csv.gz\", header=True, inferSchema=True)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(dataset.count(), (t2-t1)*1000))\n",
    "\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Data Analytics\n",
    "Let's see how the data is spread along some columns : Churn, Gender, Contract.\n",
    "\n",
    "Do you think the data has skew?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## distribution buy Chrun\n",
    "dataset.groupBy('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : Distribution by gender\n",
    "dataset.groupBy('???').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : distribution by 'Contract'\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## basic describe\n",
    "## TODO : Feel free to add more attributes to describe\n",
    "dataset.describe(['MultipleLines', 'MonthlyCharges']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define columns\n",
    "prediction = ['Churn']\n",
    "categorical = ['gender',  'InternetService','Contract','PaymentMethod']\n",
    "categorical_index = ['gender_index',  'InternetService_index','Contract_index','PaymentMethod_index']\n",
    "\n",
    "\n",
    "columns = ['SeniorCitizen','PhoneService','Partner','Dependents','tenure','MultipleLines',\n",
    "           'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n",
    "           'StreamingTV','StreamingMovies','PaperlessBilling',\n",
    "           'MonthlyCharges','TotalCharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.select(categorical).show(5)\n",
    "dataset.select(prediction).show(5)\n",
    "dataset.select(columns).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deal with Categorical Columns\n",
    "\n",
    "Let's deal with the categorical columns, including the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(categorical)\n",
    "dataset.select(categorical).show(5)\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\").\\\n",
    "            fit(dataset) for column in categorical ]\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Churn\", outputCol=\"indexedLabel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=columns + categorical_index, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Scale the features and output column \"scaledFeatures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"indexedFeatures\", outputCol=\"???\") #TODO: Fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split into training and test.\n",
    "\n",
    "**=> Split into training/test with an 80/20 split ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split into training and test\n",
    "## TODO: create training and test with an 80/20 split\n",
    "(training, test) = dataset.randomSplit([.8, .2])\n",
    "\n",
    "print(\"training set count : \", training.count())\n",
    "print(\"testing set count : \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TODO : set maxIteration to 50\n",
    "\n",
    "lsvc = LinearSVC(labelCol=\"indexedLabel\", featuresCol=\"scaledFeatures\", maxIter=???, regParam=0.1)\n",
    "\n",
    "##  with scaler\n",
    "stages = indexers + [assembler, featureIndexer, labelIndexer, scaler] \n",
    "\n",
    "## without scaler\n",
    "#stages = indexers + [assembler, featureIndexer, labelIndexer] \n",
    "\n",
    "\n",
    "i = 0\n",
    "for stage in stages:\n",
    "    i = i+1\n",
    "    print (\"stage \", i, \" : \", stage)\n",
    "print()\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "print (\"pipeline : \", pipeline.explainParams())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train  Linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "t1 = time.perf_counter()\n",
    "scaledTraining = pipeline.fit(training).transform(training)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"ran pipeline on {:,} records using {:,} stages in {:,.2f} ms\".\\\n",
    "      format(training.count(), len(stages), (t2-t1)*1000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "## TODO : supply 'scaledTraining' for fitting\n",
    "lsvcModel = lsvc.fit(???)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"trained on {:,} records using {:,} features in {:,.2f} ms\".\\\n",
    "      format(scaledTraining.count(), len(columns), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linearsSVC\n",
    "coef = lsvcModel.coefficients\n",
    "\n",
    "df = pd.DataFrame({'input' : columns + categorical_index, 'coefficient': lsvcModel.coefficients})\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "\n",
    "df\n",
    "#df.sort_values(by=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9:  Predict on Test Data\n",
    "\n",
    "**=> TODO: Transform the test dataset to get scaled Vector **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "scaledTest = pipeline.fit(test).transform(test)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"ran pipeline on {:,} records using {:,} stages in {:,.2f} ms\".\\\n",
    "      format(training.count(), len(stages), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "## TODO : create predictions on 'scaledTest' dataset\n",
    "predictions = lsvcModel.transform(???)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print(\"predicted on {:,} records in {:,.2f} ms\".\\\n",
    "      format(training.count(),  (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: See the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='indexedLabel', rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)  #AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does AUC mean?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"accuracy \", accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 : confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "predictions.groupBy('Churn').pivot('prediction', [0,1]).count().na.fill(0).orderBy('Churn').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: What is the meaning of the confusion matrix? **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 11: Try running without scaling features\n",
    "\n",
    "In Step-5  we are adding a scaler at the end to normalize the vector.  \n",
    "Try without scaler.  \n",
    "\n",
    "Uncomment the following line   \n",
    "#stages = indexers + [assembler, featureIndexer, labelIndexer] \n",
    "\n",
    "And run the whole notebook (Cell --> Run All)  \n",
    "Do you see any improvement/degradation in accuracy / AUC ?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

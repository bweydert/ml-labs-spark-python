{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md) \n",
    "\n",
    "# Lab 2.1 : Up and Running With Spark\n",
    "\n",
    "### Overview\n",
    "We will be running Spark in a single node mode.\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20 mins\n",
    "\n",
    "## STEP 0: To Instructor\n",
    "Please go through this lab on 'screen' first.\n",
    "\n",
    "## STEP 1: Login to your Spark node\n",
    "Instructor will provide details\n",
    "\n",
    "\n",
    "## STEP 2: Installing Spark\n",
    "**Skip this step, if you had already installed Spark**\n",
    "\n",
    "There is no 'install'.  Just unzip/untar and run :-)  \n",
    "(copy paste the following commands on terminal,  do not include $ in your commands)\n",
    "\n",
    "```bash\n",
    "$   cd\n",
    "$   rm -rf  spark   # cleanup existing spark installation (if any)\n",
    "$   tar xvf files/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "$   mv  spark-2.3.0-bin-hadoop2.7    spark\n",
    "```\n",
    "\n",
    "Now we have spark installed in  `~/spark`  directory\n",
    "\n",
    "\n",
    "## STEP 3: Running Spark\n",
    "\n",
    "```bash\n",
    "$   ~/spark/sbin/start-all.sh\n",
    "```\n",
    "\n",
    "Verify Spark is running by 'jps' command\n",
    "\n",
    "```bash\n",
    "$  jps\n",
    "```\n",
    "\n",
    "Your output may look like this..\n",
    "\n",
    "```  \n",
    "  30624 Jps\n",
    "  30431 Master\n",
    "  30565 Worker\n",
    "```\n",
    "\n",
    "you will see **Master** and **Worker**  processes running.  \n",
    "(you probably will get different values for process ids - first column )\n",
    "\n",
    "Spark UI will be at port 8080 of the host.  \n",
    "In browser go to  \n",
    "  http://your_spark_host_address:8080  \n",
    "(be sure to use the 'public' ip address)\n",
    "\n",
    "bingo!  Now we have spark running.\n",
    "\n",
    "\n",
    "## STEP 4: Exploring Spark UI\n",
    "You will see a similar screen shot like this\n",
    "\n",
    "<img src=\"../assets/images/1a.png\" style=\"border: 5px solid grey ; max-width:100%;\" /> \n",
    "\n",
    "To explore:\n",
    "* Is Master and Worker running on the same node?\n",
    "\n",
    "* Inspect memory & CPU available for Spark worker\n",
    "\n",
    "* Note the Spark master URI, it will be something like\n",
    "      spark://host_name:7077\n",
    "    We will need this for later labs\n",
    "\n",
    "\n",
    "## STEP 5: Download Spark labs and Data\n",
    "\n",
    "### Labs\n",
    "```bash\n",
    "$   cd\n",
    "$   git clone   git@github.com:elephantscale/spark-labs.git\n",
    "\n",
    "# for v2 - latest\n",
    "# $  cd  ~/spark-labs\n",
    "# $  git checkout master\n",
    "\n",
    "# for 1.6 (old)\n",
    "# $  cd ~/spark-labs\n",
    "# $  git checkout v1.6\n",
    "```\n",
    "\n",
    "This will create a `spark-labs` directory that has all the labs.\n",
    "\n",
    "### Data\n",
    "Download the dataset\n",
    "```bash\n",
    "    # if  ~/data dir is missing, do the following\n",
    "    # $   git clone --depth 1  git@github.com:elephantscale/datasets.git\n",
    "\n",
    "    $   cd ~/data\n",
    "    $   git pull\n",
    "```\n",
    "\n",
    "[Link to Full Dataset](https://s3.amazonaws.com/elephantscale-public/data/datasets.zip)\n",
    "(Note: Large download, ~200 Meg)\n",
    "\n",
    "- Click the above link to download or\n",
    "- use `wget` from command line\n",
    "```\n",
    "    $    wget   \"https://s3.amazonaws.com/elephantscale-public/data/datasets.zip\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Optional: Spark Sandbox\n",
    "To run Spark on your laptop, we recommend the sandbox.  This Sandbox can run on Mac / Windows / Linux.\n",
    "\n",
    "- [Download Sandbox](https://github.com/elephantscale/sandbox)\n",
    "- [Tutorials](https://github.com/elephantscale/sandbox)\n",
    "\n",
    "\n",
    "## Optional 2: Running Spark on Windows\n",
    "\n",
    "If one has to do a Windows install, here is the magic\n",
    "\n",
    "http://nishutayaltech.blogspot.com/2015/04/how-to-run-apache-spark-on-windows7-in.html\n",
    "\n",
    "set winutils as described\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "   \\projects\n",
    "   \\projects\\spark-labs\n",
    "   \\projects\\spark-1.4.1-bin-hadoop2.4\n",
    "   \\projects\\winutils\\bin\\winutils\n",
    "```\n",
    "\n",
    "System env variable\n",
    "\n",
    "```\n",
    "   HADOOP_HOME=\\projects\\winutils\n",
    "```\n",
    "\n",
    "With this download spark-1.4.1-bin-hadoop2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

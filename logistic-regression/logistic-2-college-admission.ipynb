{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression in Spark  - College Admission\n",
    "\n",
    "### Overview\n",
    "Predict college admission using Multiple Logistic Regression\n",
    " \n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "approx. 10-20 minutes\n",
    "\n",
    "### Notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: College Admission Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the college admission data.  Here, we have some student test scores, GPA, and Rank, followed by whether the student was admitted or not.\n",
    "\n",
    "```\n",
    "|gre  |gpa  |rank |  admitted |\n",
    "|-----------------------------|\n",
    "|380  |3.61 | 3   |    no     |\n",
    "|660  |3.67 | 1   |    yes    |\n",
    "|800  |4.0  | 1   |    yes    |\n",
    "|640  |3.19 | 4   |    yes    |\n",
    "|520  |2.93 | 4   |    no     |\n",
    "|760  |3.0  | 2   |    yes    |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "admissions = spark.read.csv(\"/data/college-admissions/admission-data.csv\",\\\n",
    "                            header=True, inferSchema=True)\n",
    "admissions.printSchema()\n",
    "admissions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data spread\n",
    "admissions.groupBy(\"admit\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see each column \n",
    "admissions.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare feature vector\n",
    "\n",
    "We need to firstconvert the dataframe to spark, and then prepare the feature vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO: Select all columns except for \"admit\" to be in features **\n",
    "## Hint : select columns 'gpa', 'gre', 'rank'\n",
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\",\"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Make a new column called \"label\" with same value as \"admit\"\n",
    "## Hint : featureVector is 'admit'\n",
    "featureVector = featureVector.withColumn(\"label\",featureVector[\"???\"])\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split Data into training and Test\n",
    "\n",
    "We will split our data into training and test so we can see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Split the data into train and test 70%  and 30%\n",
    "## Hint : \n",
    "##     - training split is 70%  --> 0.7\n",
    "##     - testing split  is 30%  --> 0.3\n",
    "(train, test) = featureVector.randomSplit([???,  ???])\n",
    "#(train, test) = featureVector.randomSplit([???,  ???], seed=1)\n",
    "\n",
    "## print out record count\n",
    "print (\"train dataset count : \" , train.???())\n",
    "print (\"test dataset count : \" , test.???())\n",
    "\n",
    "print(\"training data set\")\n",
    "train.show(10)\n",
    "\n",
    "print(\"test data set\")\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "## TODO : set maxIter to 10\n",
    "#lr = LogisticRegression(maxIter=???)\n",
    "lr = LogisticRegression(maxIter=???, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "## TODO : Fit the model on 'training' set\n",
    "lrModel = lr.fit(???)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Q : Do any of your coefficients zero?  What is the implication?  \n",
    "If you see ZERO coefficients, increase the number of iterations and run again**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Predict on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : predict on 'test' data\n",
    "predictions = lrModel.transform(???)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a few 0 and 1\n",
    "predictions.sampleBy(\"prediction\", fractions={0: 0.5, 1: 0.5}, seed=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('admit').pivot('prediction').count().na.fill(0).orderBy('admit').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 :  ROC Curve & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under Curve is part of training summary\n",
    "# use TAB completion :  trainingSummary.TAB\n",
    "\n",
    "print(\"areaUnderROC: \" , trainingSummary.???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "\n",
    "roc_df = trainingSummary.roc.toPandas()\n",
    "\n",
    "plt.plot(roc_df['FPR'], roc_df['TPR'])\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Iterations and Objective History\n",
    "**==> Q : how many iterations did we do?**  \n",
    "- What does that tell you?\n",
    "- Increase the total number of iterations from 50 to 100, Does it change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trainingSummary has an attribute called 'totalIterations'  \n",
    "## Hint : use the TAB completion\n",
    "print (\"total iterations \", trainingSummary.???)\n",
    "\n",
    "## you can uncomment this and see how the error is diminishing in each iteration\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in trainingSummary.objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: transform the test data\n",
    "## HINT : test\n",
    "predictions = lrModel.transform(???)\n",
    "predictions.show()\n",
    "\n",
    "## sample\n",
    "predictions.sampleBy(\"label\", fractions={0: 0.5, 1: 0.5}, seed=0).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calcuate Accuracy on Test Data\n",
    "\n",
    "Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run some predictions on new data\n",
    "\n",
    "Let's take some new data, and run predictions on that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame({'gre' : [600, 700, 800], \n",
    "                        'gpa' : [4.0, 3.5, 3.2],\n",
    "                        'rank': [1,   2,   3]}\n",
    "             )\n",
    "print(newdata)\n",
    "\n",
    "\n",
    "## TODO: create spark dataframe from pandas dataframe\n",
    "## Hint : input is 'newdata'\n",
    "spark_newdata = spark.createDataFrame(???)\n",
    "\n",
    "## TODO: transform the new data in order to get feature vectors\n",
    "## Hint : spark_newdata\n",
    "newfeatures = assembler.transform(???)\n",
    "\n",
    "lrModel.transform(newfeatures).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Save Output\n",
    "**TODO : Inspect the saved data**  \n",
    "(Hint : you can open them in excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to a csv file for inspection\n",
    "predictions2 = predictions.select(['admit', 'gre', 'gpa', 'rank', 'prediction'])\n",
    "\n",
    "## option1 : use Spark write function\n",
    "## this works for big data (writes are distributed across cluster)\n",
    "output_path1=\"college-admissions-predictions.out1\"\n",
    "predictions2.write.\\\n",
    "    option('header', 'true').\\\n",
    "    mode('overwrite').\\\n",
    "    csv(output_path1)\n",
    "print(\"save 1 (spark)  to : \", output_path1)\n",
    "\n",
    "\n",
    "## Option 2 : convert to Pandas dataframe and save\n",
    "## This is good for small amount of data\n",
    "output_path2= 'college-admissions-predictions.out2.csv'\n",
    "predictions2.toPandas().to_csv(output_path2 )\n",
    "print(\"save 2 (pandas) to : \", output_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "# Area Under Curve\n",
    "print(\"areaUnderROC: \" , trainingSummary.areaUnderROC)\n",
    "accuracy = evaluator.evaluate(predictions) \n",
    "print(\"Test set accuracy = \" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 - Why do you think the accuracy varies for each run?  \n",
    "\n",
    "Try this, at Step-3, supply a seed parameter (can be any number) and do the run again.  \n",
    "Do you see the accuracy varying now?  \n",
    "Can you explain the behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 - Adjust regularization (lambda) parameter\n",
    "Vary 'regparam' in **step 4** from 0.1 to 0.9.  \n",
    "Do you see accuracy changing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 - Set threshold\n",
    "We can setup 'threshold' in Step-4, as follows\n",
    "\n",
    "```python\n",
    "lr = LogisticRegression(maxIter=???, regParam=0.3, elasticNetParam=0.8)\n",
    "lr.threshold = 0.7  # values between 0 and 1.0\n",
    "```\n",
    "\n",
    "Change the threshold and try a few runs.  See how the accuracy changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 - Class discussion\n",
    "How do you find the right 'lambda' ..etc values?  \n",
    "Hint : lots of experimental runs :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

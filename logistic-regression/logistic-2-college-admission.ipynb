{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression in Spark  - College Admission\n",
    "\n",
    "### Overview\n",
    "Predict college admission using Multiple Logistic Regression\n",
    " \n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "approx. 10-20 minutes\n",
    "\n",
    "### Notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: College Admission Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the college admission data.  Here, we have some student test scores, GPA, and Rank, followed by whether the student was admitted or not.\n",
    "\n",
    "\n",
    "|gre  |gpa  |rank |  admitted |\n",
    "|-----------------------------|\n",
    "|380  |3.61 | 3   |    no     |\n",
    "|660  |3.67 | 1   |    yes    |\n",
    "|800  |4.0  | 1   |    yes    |\n",
    "|640  |3.19 | 4   |    yes    |\n",
    "|520  |2.93 | 4   |    no     |\n",
    "|760  |3.0  | 2   |    yes    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admissions = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "admissions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert dataframe to Spark and Prepare feature vector\n",
    "\n",
    "We need to firstconvert the dataframe to spark, and then prepare the feature vector.\n",
    "\n",
    "**=> TODO: Select all columns except for \"admit\" to be in features **\n",
    "\n",
    "**=> TODO: Make a new column called \"label\" with same value as \"admit\" **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hint : select columns 'gpa', 'gre', 'rank'\n",
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\",\"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(admissions)\n",
    "\n",
    "## Hint : featureVector is 'admit'\n",
    "featureVector = featureVector.withColumn(\"label\",featureVector[\"???\"])\n",
    "featureVector.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split Data into training and Test\n",
    "\n",
    "We will split our data into training and test so we can see how it performs.\n",
    "\n",
    "**=> TODO: Use training / test split of 70%/30% **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the data into train and test\n",
    "## Hint : \n",
    "##     - training split is 70%  --> 0.7\n",
    "##     - testing split  is 30%  --> 0.3\n",
    "(train, test) = featureVector.randomSplit([??training_split??,  ??testing split??])\n",
    "#(train, test) = featureVector.randomSplit([??training_split??,  ??testing split??], seed=1)\n",
    "\n",
    "## print out record count\n",
    "print (\"train dataset count : \" , train.???())\n",
    "print (\"test dataset count : \" , test.???())\n",
    "\n",
    "print(\"training data set\")\n",
    "train.show(10)\n",
    "\n",
    "print(\"test data set\")\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Run logistic regression\n",
    "\n",
    "**=> TODO: Run with 50 iteraitons **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=???, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## show predictions in training data\n",
    "lrModel.summary.predictions.show()\n",
    "\n",
    "## sample based on data\n",
    "lrModel.summary.predictions.sampleBy(\"label\", fractions={0: 0.5, 1: 0.5}, seed=0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## How many data points where label != prediction\n",
    "missed = lrModel.summary.predictions.filter(\"label != prediction\")\n",
    "\n",
    "print(\"missed \", missed.count())\n",
    "missed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.groupBy('admit').pivot('prediction').count().na.fill(0).orderBy('admit').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 :  ROC Curve & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Area Under Curve is part of training summary\n",
    "# use TAB completion :  trainingSummary.TAB\n",
    "\n",
    "print(\"areaUnderROC: \" , trainingSummary.???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC\n",
    "\n",
    "roc_df = trainingSummary.roc.toPandas()\n",
    "\n",
    "plt.plot(roc_df['FPR'], roc_df['TPR'])\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Iterations and Objective History\n",
    "** Q : how many iterations did we do?**  \n",
    "- What does that tell you?\n",
    "- Increase the total number of iterations from 50 to 100, Does it change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## trainingSummary has an attribute called 'totalIterations'  \n",
    "## Hint : use the TAB completion\n",
    "print (\"total iterations \", trainingSummary.???)\n",
    "\n",
    "## you can uncomment this and see how the error is diminishing in each iteration\n",
    "print(\"objectiveHistory:\")\n",
    "#for objective in trainingSummary.objectiveHistory:\n",
    "#    print(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run on the test data\n",
    "\n",
    "**=>TODO: transform the test data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## what is the name of test dataframe?\n",
    "predictions = lrModel.transform(???)\n",
    "predictions.show()\n",
    "\n",
    "## sample\n",
    "predictions.sampleBy(\"label\", fractions={0: 0.5, 1: 0.5}, seed=0).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calcuate Accuracy on Test Data\n",
    "\n",
    "**=>TODO: evaluate the predictions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run some predictions on new data\n",
    "\n",
    "Let's take some new data, and run predictions on that.\n",
    "\n",
    "**=>TODO: create spark dataframe from pandas dataframe **\n",
    "\n",
    "**=>TODO: transform the new data in order to get feature vectors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame({'gre' : [600, 700, 800], \n",
    "                        'gpa' : [4.0, 3.5, 3.2],\n",
    "                        'rank': [1,   2,   3]}\n",
    "             )\n",
    "print(newdata)\n",
    "\n",
    "## Hint : input is 'newdata'\n",
    "spark_newdata = spark.createDataFrame(???)\n",
    "\n",
    "## Hint : spark_newdata\n",
    "newfeatures = assembler.transform(???)\n",
    "\n",
    "lrModel.transform(newfeatures).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Save Output\n",
    "**TODO : Inspect the saved data**  \n",
    "(Hint : you can open them in excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save data to a csv file for inspection\n",
    "predictions2 = predictions.select(['admit', 'gre', 'gpa', 'rank', 'prediction'])\n",
    "\n",
    "## option1 : use Spark write function\n",
    "## this works for big data (writes are distributed across cluster)\n",
    "output_path1=\"college-admissions-predictions.out1\"\n",
    "predictions2.write.\\\n",
    "    option('header', 'true').\\\n",
    "    mode('overwrite').\\\n",
    "    csv(output_path1)\n",
    "print(\"save 1 (spark)  to : \", output_path1)\n",
    "\n",
    "\n",
    "## Option 2 : convert to Pandas dataframe and save\n",
    "## This is good for small amount of data\n",
    "output_path2= 'college-admissions-predictions.out2.csv'\n",
    "predictions2.toPandas().to_csv(output_path2 )\n",
    "print(\"save 2 (pandas) to : \", output_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Do a few runs and see the accuracy\n",
    "Why do you think the accuracy varies for each run?  \n",
    "\n",
    "Try this, at Step-3, supply a seed parameter (can be any number) and do the run again.  \n",
    "Do you see the accuracy varying now?  \n",
    "Can you explain the behaviour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "# Area Under Curve\n",
    "print(\"areaUnderROC: \" , trainingSummary.areaUnderROC)\n",
    "accuracy = evaluator.evaluate(predictions) \n",
    "print(\"Test set accuracy = \" , accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

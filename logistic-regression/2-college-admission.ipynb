{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Logistic Regression in Spark  - College Admission\n",
    "\n",
    "### Overview\n",
    "Predict college admission using Multiple Logistic Regression\n",
    " \n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "approx. 10-20 minutes\n",
    "\n",
    "### Notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: College Admission Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the college admission data.  Here, we have some student test scores, GPA, and Rank, followed by whether the student was admitted or not.\n",
    "\n",
    "\n",
    "|gre  |gpa  |rank |  admitted |\n",
    "|-----------------------------|\n",
    "|380  |3.61 | 3   |    no     |\n",
    "|660  |3.67 | 1   |    yes    |\n",
    "|800  |4.0  | 1   |    yes    |\n",
    "|640  |3.19 | 4   |    yes    |\n",
    "|520  |2.93 | 4   |    no     |\n",
    "|760  |3.0  | 2   |    yes    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "admissions = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "admissions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a 4D plot !\n",
    "\n",
    "We will use a 3d plot, and encode the fourth dimension as color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_pd = admissions.toPandas()\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.random.standard_normal(100)\n",
    "y = np.random.standard_normal(100)\n",
    "z = np.random.standard_normal(100)\n",
    "c = np.random.standard_normal(100)\n",
    "\n",
    "ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert dataframe to Spark and Prepare feature vector\n",
    "\n",
    "We need to firstconvert the dataframe to spark, and then prepare the feature vector.\n",
    "\n",
    "**=> TODO: Select all columns except for \"admit\" to be in features **\n",
    "\n",
    "**=> TODO: Make a new column called \"label\" with same value as \"admit\" **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\",\"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(admissions)\n",
    "featureVector = featureVector.withColumn(\"label\",featureVector[\"???\"])\n",
    "featureVector.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Data into training and Test\n",
    "\n",
    "We will split our data into training and test so we can see how it performs.\n",
    "\n",
    "**=> TODO: Use training / test split of 60%/40% **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the data into train and test\n",
    "## Hint : \n",
    "##     - training split is 70%  --> 0.7\n",
    "##     - testing split  is 30%  --> 0.3\n",
    "(train, test) = featureVector.randomSplit([??training_split??,  ??testing split??], seed=1)\n",
    "\n",
    "## print out record count\n",
    "print (\"train dataset count : \" , train.???())\n",
    "print (\"test dataset count : \" , test.???())\n",
    "\n",
    "print(\"training data set\")\n",
    "train.show(10)\n",
    "\n",
    "print(\"test data set\")\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Run logistic regression\n",
    "\n",
    "**=> TODO: Run with 50 iteraitons **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(maxIter=???, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.summary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output lists approval & estimated probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "print (\"total iterations \", trainingSummary.totalIterations)\n",
    "\n",
    "# Area Under Curve\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "\n",
    "roc_df = trainingSummary.roc.toPandas()\n",
    "\n",
    "plt.plot(roc_df['FPR'], roc_df['TPR'])\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run on the test data\n",
    "\n",
    "**=>TODO: transform the test data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 4: Run on the test data\n",
    "\n",
    "predictions = lrModel.transform(??test data??) #TODO: Complete this step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calcuate Accuracy on Test Data\n",
    "\n",
    "**=>TODO: evaluate the predictions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(???predictions???)   #TODO: Complete this step\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run some predictions on new data\n",
    "\n",
    "Let's take some new data, and run predictions on that.\n",
    "\n",
    "**=>TODO: create spark dataframe from pandas dataframe **\n",
    "**=>TODO: transform the new data in order to get feature vectors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame({'gre' : [600, 700, 800], \n",
    "                        'gpa' : [4.0, 3.5, 3.2],\n",
    "                        'rank': [1,   2,   3]}\n",
    "             )\n",
    "print(newdata)\n",
    "\n",
    "spark_newdata = spark.createDataFrame(???)  #TODO: Complete this step\n",
    "newfeatures = assembler.transform(???)  #TODO: Complete this step\n",
    "lrModel.transform(newfeatures).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

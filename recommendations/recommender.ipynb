{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations Lab Using ALS: Audioscrobble\n",
    "\n",
    "## Description\n",
    "\n",
    "We are going to be using the Audioscrobbler dataset and Spark's ALS recommendation system\n",
    "using collaborative filtering. \n",
    "\n",
    "\n",
    "## Datasets\n",
    "\n",
    "You will be using some publicly available song data from audioscrobbler, which can be found [here](http://www-etud.iro.umontreal.ca/~bergstrj/audioscrobbler_data.html). However, we modified the original data files so that the code will run in a reasonable time on a single machine. The reduced data files have been suffixed with `_small.txt` and contains only the information relevant to the top 50 most prolific users (highest artist play counts).\n",
    "\n",
    "The original data file `user_artist_data.txt` contained about 141,000 unique users, and 1.6 million unique artists. About 24.2 million usersâ€™ plays of artists are recorded, along with their count.\n",
    "\n",
    "Note that when plays are scribbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard, and this may only be detected later. For example, \"The Smiths\", \"Smiths, The\", and \"the smiths\" may appear as distinct artist IDs in the data set, even though they clearly refer to the same artist. So, the data set includes `artist_alias.txt`, which maps artist IDs that are known misspellings or variants to the canonical ID of that artist.\n",
    "\n",
    "The `artist_data.txt` file then provides a map from the canonical artist ID to the name of the artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recommender Model\n",
    "\n",
    "For this project, we will train the model with implicit feedback. You can read more information about this from the collaborative filtering page: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html). The [function you will be using](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS.trainImplicit) has a few tunable parameters that will affect how the model is built. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading data\n",
    "\n",
    "Load the data into dataframes: artist and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists = spark.read.format(\"csv\").option('header','true').option('delimiter', '\\t').\\\n",
    "  option('inferSchema', 'true').load(\"/data/audioscrobble/artist_data.txt.gz\")\n",
    "\n",
    "dataset = spark.read.csv(\"/data/audioscrobble/user_artist_data.csv.gz\", header=True, inferSchema=True)\n",
    "\n",
    "(training, test) = dataset.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter some sample data.\n",
    "\n",
    "Just for fun, we are going to make our own imaginary user playlist, and see what is \n",
    "recommended to that user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classic Rock Fan Data\n",
    "\n",
    "pd_df = pd.DataFrame({'User' : [99999, 99999, 99999, 99999, 99999, 99999, 99999],\n",
    "                      'Artist' : [10215385, 9915421, 3292, 5687, 1014221, 1000055,  1004241],\n",
    "                      'Count' : [12, 7, 13, 8, 15, 5, 2]\n",
    "             })\n",
    "\n",
    "my_playlist = spark.createDataFrame(pd_df)\n",
    "\n",
    "training = training.unionAll(my_playlist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You enter your own favorites\n",
    "\n",
    "Open your music player or phone and see what some of your favorites are in your playlists. Then, find the artist ids from the artist file and create your own playlist.\n",
    "\n",
    "**=> TODO: Create your own dataframe with your own favorites\n",
    "\n",
    "You can grep for your favorite artists in data/audioscrobble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Your data\n",
    "\n",
    "# Use User #99998\n",
    "\n",
    "# Create a pandas dataframe with your data.  Look up your data from the artists dataframe\n",
    "\n",
    "\n",
    "# add it to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artists.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Train ALS model using implicit ratings.\n",
    "\n",
    "We are going to use the ALS model using implicit ratings.  That means that the playcount\n",
    "will be more of a rough guide to how much the user likes the music rather than an explicit \n",
    "1-5 star rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"User\", itemCol=\"Artist\", ratingCol=\"PlayCount\",\n",
    "          coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulate the model\n",
    "\n",
    "The problem with implicit ratings is that we don't have an objective measure to going back and evaulating our model.  What we're going to do here is arbitraily create a column called \"liked\"\n",
    "that we define as playcount greater than 2.  Then we'll see if the results are positive, meaning that our model predicted the user would like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "predictions_with_liked = predictions.withColumn('liked', (predictions.PlayCount > lit(2)).cast('integer'))\n",
    "predictions_with_predicted_like = predictions_with_liked.withColumn('predicted_liked', (predictions.prediction > lit(.01)).cast('integer'))\n",
    "predictions_with_predicted_like = predictions_with_predicted_like.withColumn('raw_prediction', predictions.prediction.cast('double'))\n",
    "predictions_with_predicted_like.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Recommendations for all users\n",
    "\n",
    "Note that this will take a while. Be Patient.\n",
    "\n",
    "Once you're done, you can check out the classic rock fan, user 99999, and your own, user 99998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the recomendations for the Classic Rock Fan, User 99999\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userRecs.filter(userRecs.User == 99999).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Show your own recommendations**\n",
    "\n",
    "Print out receommendations for your user 99998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: show your own recomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"PlayCount\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out AUC for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"raw_prediction\", labelCol=\"liked\")\n",
    "evaluator.evaluate(predictions_with_predicted_like)  #AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Your own evalutation\n",
    "\n",
    "Can you think of some of your own evaluation metrics that you can run?\n",
    "\n",
    "**=> TODO: Look at some other evaluation metrics**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

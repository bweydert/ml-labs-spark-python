{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Presidential Election Contributions\n",
    "\n",
    "Let's clean up the presidential contributions dataset by feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "from pyspark.sql.functions import isnan, when, count, col, split, trim, countDistinct, abs \n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "In US contributions to elections are public record published by Federal Election Commission (FEC).   \n",
    "Here we have a sample of 10k for 2016 contributions.  \n",
    "\n",
    "Optionally, To download the full dataset, you can execute the **download-data.sh** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t1 = time.perf_counter()\n",
    "#Load presidential contrib data\n",
    "contribs = spark.read.csv(\"/data/presidential_election_contribs/2016/2016-100k.csv.gz\",\\\n",
    "                          header=True, inferSchema=True)\n",
    "t2 = time.perf_counter()\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(contribs.count(), (t2-t1)*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: printSchema\n",
    "contribs.???\n",
    "\n",
    "## TODO : how many records do we have?\n",
    "## Hint : count\n",
    "print (contribs.???())\n",
    "\n",
    "## TODO : see some sample data\n",
    "## HInt : show()\n",
    "contribs.???()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Select fields to analyze\n",
    "The dataset has quite a few fields.  Let's start with some obvious ones.  \n",
    "YOu can always add more fields later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : extract the following fields\n",
    "##    - Candidate Name : 'CAND_NM\n",
    "##    - Contributor Name : 'CONTBR_NM'\n",
    "##    - Contributor Occupation : ????\n",
    "##    - COntribution Amount : ????\n",
    "##    - contribution zip code : ???\n",
    "\n",
    "contribs2 = contribs.select([ 'CAND_NM', 'CONTBR_NM', '???', '???', '???'])\n",
    "print(contribs2.count())\n",
    "contribs2.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Register SQL tables\n",
    "For ease of analytics let's also register these as SQL temp tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hint : use 'createOrReplaceTempView' function\n",
    "contribs.???(\"contribs\")\n",
    "contribs2.???(\"contribs2\")\n",
    "\n",
    "## list tables\n",
    "## Hint : Use function in spark.catalog.\n",
    "## Use tab completion\n",
    "spark.catalog.???()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Negative Contributions ??!??\n",
    "\n",
    "As a cleanup, let us check for contribution amounts less than zero, and fix if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contribution receipt less than zero?\n",
    "## TODO : filter\n",
    "## HINT : filter ('column name for amount' < 0)\n",
    "#negative_contrib = contrib2.???(' condition ')\n",
    "negative_contrib = contribs2.filter('???')\n",
    "\n",
    "## TODO : how many negative contributions do we have?\n",
    "## Hint : count\n",
    "print(negative_contrib.count())\n",
    "\n",
    "## TODO : print some sample data\n",
    "## Hint : show()\n",
    "negative_contrib.show(10, False)\n",
    "\n",
    "## TODO : Now try the query in SQL\n",
    "#neg = spark.sql(\"your sql query to find negative contribution goes here\")\n",
    "#neg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Extract data that only has positive contribution amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: \n",
    "##     extract only positive  contribs\n",
    "pos_contribs = contribs2.filter('???')\n",
    "\n",
    "print(pos_contribs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : in SQL\n",
    "pos_contribs2 = spark.sql(\"query for possitive contributions go here\")\n",
    "pos_contribs2.show()\n",
    "\n",
    "## now register this as a new table\n",
    "pos_contribs2.createOrReplaceTempView(\"pos_contribs\")\n",
    "\n",
    "## TODO : count how many postivive contribs \n",
    "spark.sql(\"???\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split up name into first name and last name\n",
    "This is an example on how to create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_col = split(contribs2['CONTBR_NM'], ',')\n",
    "first_last_name = contribs2.withColumn('LASTNAME', trim(split_col.getItem(0)))\n",
    "first_last_name = first_last_name.withColumn('FIRSTNAME', trim(split_col.getItem(1)))\n",
    "\n",
    "first_last_name.show()\n",
    "\n",
    "## register as sql table\n",
    "first_last_name.createOrReplaceTempView(\"pos_contribs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze data\n",
    "We can use Dataframe DSL language or SQL queries to perform analysis.  \n",
    "Practice both.   \n",
    "We will give you hints along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Count contributions per candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : What is the breakdown by candidate name?\n",
    "## Hint : groupBy(\"name\").count()\n",
    "\n",
    "pos_contribs.groupBy(\"???\").count().show()\n",
    "\n",
    "\n",
    "## TODO : try SQL queries\n",
    "\n",
    "s=\"\"\"\n",
    "select CAND_NM, ????\n",
    "????\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()\n",
    "\n",
    "\n",
    "## TODO : sort the result by contrib total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Calculate AVG, MIN, MAX contributions per candidate\n",
    "For this we can easily use SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "s=\"\"\"\n",
    "select CAND_NM, AVG(???)  as avg_contrib , MAX(???) as max_contrib\n",
    "from pos_contribs\n",
    "group by ??? order by ???  DESC\n",
    "\"\"\"\n",
    "\n",
    "avg_per_candidate = spark.sql(\"\")\n",
    "avg_per_candidate.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 - Calculate AVG contribution by Occupation\n",
    "If you are political consultants, which people you might try to solicit money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "#avg_per_occupation = spark.sql(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 - Find Zipcodes that give most money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : Use a sql query\n",
    "## Hint : you want to calcluate average contribution per zip\n",
    "## also calculate number of contribs per zipcode\n",
    "\n",
    "s=\"\"\"\n",
    "write your sql here\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hint : to extract 5 digit zipcode \n",
    "## you can use : substring(CONTBR_ZIP, 0, 5) as zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion\n",
    "Now that we have done some feature engineering, waht other attributes we can extract and analyze?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

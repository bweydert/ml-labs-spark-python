{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "# Linear Regression in PySpark (Demo)\n",
    "\n",
    "### Overview\n",
    "Instructor to demo this on screen.\n",
    " \n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "approx. 20-30 minutes\n",
    "\n",
    "### Notes\n",
    "\n",
    "PySpark has a class called Linear Regression that can be used to do simple linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : Tips\n",
    "Here is our tip data.  This shows 10 observations of bill with tip amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| bill | tip | \n",
    "|------|-----| \n",
    "| 50   | 12  | \n",
    "| 30   | 7   | \n",
    "| 60   | 13  | \n",
    "| 40   | 8   | \n",
    "| 65   | 15  | \n",
    "| 20   | 5   | \n",
    "| 10   | 2   | \n",
    "| 15   | 2   | \n",
    "| 25   | 3   | \n",
    "| 35   | 4   | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's create a Pandas dataframe with the data\n",
    "\n",
    "Note that pandas dataframes are not the same as spark dataframes.  However, Pandas dataframes are ubiquitous in python (for small datasets) and are a nifty way to ensure that we have our data's schema match perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "tip_data = pd.DataFrame({'bill' : [50.00, 30.00, 60.00, 40.00, 65.00, 20.00, 10.00, 15.00, 25.00, 35.00],\n",
    "              'tip' : [12.00, 7.00, 13.00, 8.00, 15.00, 5.00, 2.00, 2.00, 3.00, 4.00]\n",
    "             })\n",
    "\n",
    "tip_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Let's do a quick plot of the data\n",
    "\n",
    "Let us use matplotlib to do a quick scatter plot of the data. Note that this is not a Spark dataframe, this is using the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spark Dataframe\n",
    "\n",
    "Let's now take our pandas dataframe and make a spark dataframe out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_tips = spark.createDataFrame(tip_data)\n",
    "spark_tips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a Features column with the Vector\n",
    "\n",
    "We need to create a feature vector here.  You might wonder why we need a \"vector\" after all there's only one variable: \"bill.\"  Nonetheless, we still need the dataframe to have a column called \"features\" of type Vector Double).\n",
    "\n",
    "Luckily, there's a handy class called VectorAssembler that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"bill\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(spark_tips)\n",
    "\n",
    "featureVector.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Label column with the Vector\n",
    "\n",
    "We need a way to have the outcome variable.  In this case, we are trying to predict the tip from the bill.  So, we need to signal to Spark MLlib which one is the outcome variable.  In this case, we'll create a new variable called label.  Since we already have that, we can just rename the \"tip\" column as the \"label\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureVector = featureVector.withColumnRenamed(\"tip\", \"label\")\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/images/tips3.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n",
    "## Step 6: Run Linear Regression in Spark\n",
    "\n",
    "Let's run our linear regression.  To do this we need to run the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(featureVector)\n",
    "\n",
    "\n",
    "intercept = lrModel.intercept  # This is the intercept\n",
    "slope = lrModel.coefficients[0] #This is the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plot the fit line (abline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of values in the best fit line\n",
    "abline_values = [slope * i + intercept for i in tip_data.bill]\n",
    "\n",
    "# Plot the best fit line over the actual values\n",
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.plot(tip_data.bill, abline_values, 'b')\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Fit Line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print out the Outputs\n",
    "\n",
    "Here is a sample output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Plot the residuals\n",
    "\n",
    "Residuals are the error, or difference between the model predicted and model actual.  We'd like these to be as small as possible, with residuals roughly balanced.   We don't want a model that consistently predicts values too high or too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = lrModel.summary.residuals.toPandas()\n",
    "plt.scatter(tip_data.bill, residuals['residuals'])\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # horizon\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Identify Coefficients\n",
    "\n",
    "### Intercept and Slope\n",
    "We can see them from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Coefficients:\n",
    "            Estimate \n",
    "(Intercept) -0.8217112049846651\n",
    "bill        0.226334605857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Slope** (of line) : ** 0.226334605857**\n",
    "- **Intercept** (where line meets Y-axis) : **-0.8217112049846651**  (below zero line)\n",
    "\n",
    "We can also get these programatically.  \n",
    "If `tip = a * amount + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "a = lrModel.coefficients[0]\n",
    "b = lrModel.intercept\n",
    "\n",
    "intercept = lrModel.intercept\n",
    "slope = lrModel.coefficients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient (r)\n",
    "Correlation Coefficient (r) denotes how strongly X & Y are inter-related.  \n",
    "\n",
    "Spark ML can create a correlation Matrix which shows how all the variables in the dataset are correlated, if at all.  Naturally, every variable will have a correlation of 1 when compared to itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"bill\", \"tip\"], outputCol=\"features\")\n",
    "correlationVector = assembler.transform(spark_tips)\n",
    "\n",
    "r1 = Correlation.corr(correlationVector, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==>  Question : Does bill amount influence tip amount? (are they strongly linked?) **\n",
    "\n",
    "\n",
    "## Coefficient of Determination (r&sup2;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# coefficient.of.determination (r^2) \n",
    "rsquared = lrModel.summary.r2\n",
    "# 0.9067141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==>  Question : What does r&sup2; tells us? **\n",
    "\n",
    "## Step 11: Calculate tip for $100 bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "tip_for_100 = a * 100 + b   \n",
    "print(tip_for_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Add a estimated_tip column to pandas and spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "tip_data['est_tip'] = tip_data.bill * a + b\n",
    "\n",
    "tip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add estimated tip column to pyspark dataframe\n",
    "\n",
    "This is a bit tricky. We need to use the sql expr function to make this work.\n",
    "\n",
    "The formula: (bill * a) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "formula = \"(bill * \" + str(a) + \") + \" + str(b)\n",
    "\n",
    "print(formula)\n",
    "## Step 6: Add estimated tip column to spark dataframe\n",
    "spark_tips_with_est = spark_tips.withColumn(\"est_tip\", expr(formula))\n",
    "spark_tips_with_est.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Complete Code\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "# tip = a * amount + b\n",
    "\n",
    "# Make a Pandas dataframe\n",
    "tip_data = pd.DataFrame({'bill' : [50.00, 30.00, 60.00, 40.00, 65.00, 20.00, 10.00, 15.00, 25.00, 35.00],\n",
    "              'tip' : [12.00, 7.00, 13.00, 8.00, 15.00, 5.00, 2.00, 2.00, 3.00, 4.00]\n",
    "             })\n",
    "                       \n",
    "print(tip_data)\n",
    "\n",
    "#Plot the Tip Data\n",
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a Spark Dataframe\n",
    "spark_tips = spark.createDataFrame(tip_data)\n",
    "spark_tips.show()\n",
    "\n",
    "# Create a Features Columns\n",
    "assembler = VectorAssembler(inputCols=[\"bill\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(spark_tips)\n",
    "\n",
    "featureVector.show()\n",
    "\n",
    "# Create the label column\n",
    "featureVector = featureVector.withColumnRenamed(\"tip\", \"label\")\n",
    "featureVector.show()\n",
    "\n",
    "# Run Linear Regression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(featureVector)\n",
    "\n",
    "\n",
    "intercept = lrModel.intercept  # This is the intercept\n",
    "slope = lrModel.coefficients[0] #This is the slope\n",
    "\n",
    "# Plot the best fit line over the actual values\n",
    "abline_values = [slope * i + intercept for i in tip_data.bill]\n",
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.plot(tip_data.bill, abline_values, 'b')\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Fit Line\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "#Plot Residuals\n",
    "residuals = lrModel.summary.residuals.toPandas()\n",
    "plt.scatter(tip_data.bill, residuals['residuals'])\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # horizon\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Tip for $100 bill\n",
    "tip_for_100 = a * 100 + b   \n",
    "print(tip_for_100)\n",
    "\n",
    "\n",
    "# Add est_tip to dataframes\n",
    "tip_data['est_tip'] = tip_data.bill * a + b  #Pandas\n",
    "tip_data\n",
    "\n",
    "formula = \"(bill * \" + str(a) + \") + \" + str(b)  #Spark\n",
    "spark_tips_with_est = spark_tips.withColumn(\"est_tip\", expr(formula))\n",
    "spark_tips_with_est.show()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

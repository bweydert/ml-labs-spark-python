{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "# Linear Regression in PySpark (Demo)\n",
    "\n",
    "### Overview\n",
    "Instructor to demo this on screen.\n",
    " \n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "approx. 20-30 minutes\n",
    "\n",
    "### Notes\n",
    "\n",
    "PySpark has a class called Linear Regression that can be used to do simple linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : Tips\n",
    "Here is our tip data.  This shows 10 observations of bill with tip amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| bill | tip | \n",
    "|------|-----| \n",
    "| 50   | 12  | \n",
    "| 30   | 7   | \n",
    "| 60   | 13  | \n",
    "| 40   | 8   | \n",
    "| 65   | 15  | \n",
    "| 20   | 5   | \n",
    "| 10   | 2   | \n",
    "| 15   | 2   | \n",
    "| 25   | 3   | \n",
    "| 35   | 4   | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's create a Pandas dataframe with the data\n",
    "\n",
    "Note that pandas dataframes are not the same as spark dataframes.  However, Pandas dataframes are ubiquitous in python (for small datasets) and are a nifty way to ensure that we have our data's schema match perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill   tip\n",
       "0  50.0  12.0\n",
       "1  30.0   7.0\n",
       "2  60.0  13.0\n",
       "3  40.0   8.0\n",
       "4  65.0  15.0\n",
       "5  20.0   5.0\n",
       "6  10.0   2.0\n",
       "7  15.0   2.0\n",
       "8  25.0   3.0\n",
       "9  35.0   4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_data = pd.DataFrame({'bill' : [50.00, 30.00, 60.00, 40.00, 65.00, 20.00, 10.00, 15.00, 25.00, 35.00],\n",
    "              'tip' : [12.00, 7.00, 13.00, 8.00, 15.00, 5.00, 2.00, 2.00, 3.00, 4.00]\n",
    "             })\n",
    "\n",
    "tip_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Let's do a quick plot of the data\n",
    "\n",
    "Let us use matplotlib to do a quick scatter plot of the data. Note that this is not a Spark dataframe, this is using the pandas dataframe.\n",
    "\n",
    "**=>TODO: plot the bill (X-axis), versus the tip (Y-axis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEwhJREFUeJzt3W+QXfdd3/H3J0iCNUTGcTZJi5q9GMYRdSNAbRwzSctV\nUtUGBqcPIE3ShiRj6AQHkilMJo7pVPukLWWmQ0OpO5Nho0lpLRyHNHEY/jge604JrZtNbLMa/wmZ\nkrs2JBY3NJg/s2PJ+NsHe1ddr7Va7V3de+5ZvV8zOz733HPv+ex6pY9+v7Pnt6kqJEmXtxc1HUCS\n1DzLQJJkGUiSLANJEpaBJAnLQJLEmMsgyUKS00mWNuz/6SSPJTmV5OfHmUGStLU9Y37/48B/BP7L\n2o4kXeCHgVdX1bNJXjrmDJKkLYx1ZFBVnwW+vmH3TwI/X1XPDo/52jgzSJK21sQ1g2uBf5DkgSQn\nk/y9BjJIktYZ9zTRZue8qqpuSPIa4GPANQ3kkCQNNVEGTwKfAKiqxSTPJbm6qv5044FJXDhJkkZQ\nVdnO8ZOYJsrwY80ngTcAJLkW2Hu+IlhTVa39OHbsWOMZLtf8bc5u/uY/2p5/FGMdGSS5E+gCVyd5\nAjgGfAQ4nuQU8AzwY+PMIEna2ljLoKretslTbx/neSVJ2+MdyGPU7XabjrAjbc7f5uxg/qa1Pf8o\nMur80iQkqWnOJ0nTKAk1hReQJUlTzjKQJFkGkiTLQJKEZSBJwjKQpIkaDAYsLi4yGAyajvI8loEk\nTciJE3cxN3eQo0ffzdzcQU6cuKvpSOd4n4EkTcBgMGBu7iArKyeBQ8ASMzNHWF5+nNnZ2Ut6Lu8z\nkKQp1e/32bevw2oRABxi7945+v1+c6HWsQwkaQI6nQ5nzvSBtV8Jv8TZs8t0Op3mQq1jGUjSBMzO\nzrKwcAczM0fYv/8wMzNHWFi445JPEY3KawaSNEGDwYB+v0+n0xlbEYxyzcAykKRdxgvIkqSRWAaS\nJMtAkmQZSJIYcxkkWUhyOsnSeZ772STPJXnJODNIkrY27pHBceDGjTuTHACOAstjPr8k6SKMtQyq\n6rPA18/z1C8C7x/nuSVJF2/i1wyS3Aw8WVWnJn1uSdL57ZnkyZLMALezOkV0bvckM0iSXmiiZQB8\nB9ABfj9JgAPAF5JcX1V/cr4XzM/Pn9vudrt0u93xp5SkFun1evR6vR29x9iXo0jSAT5dVa8+z3Nf\nBg5X1fmuK7gchSSNYOqWo0hyJ/A/gWuTPJHkXRsOKZwmkqTGuVCdJO0yUzcykCS1g2UgSbIMJEmW\ngSQJy0CShGUgScIykKTnGQwGLC4uMhgMmo4yUZaBJA2dOHEXc3MHOXr03czNHeTEibuajjQx3nQm\nSayOCObmDrKychI4BCwxM3OE5eXHmZ2dbTretnjTmSSNqN/vs29fh9UiADjE3r1z9Pv95kJNkGUg\nSUCn0+HMmT6w9lt6lzh7dplOp9NcqAmyDCQJmJ2dZWHhDmZmjrB//2FmZo6wsHBH66aIRuU1A0la\nZzAY0O/36XQ6rS2CUa4ZWAaStMt4AVmSNBLLQJJkGUiSLANJEpaBJIkxl0GShSSnkyyt2/cLSR5L\n8nCSX0+yf5wZJElbG/fI4Dhw44Z99wLXVdX3AF8CPjjmDJIm6HJd9bPtxloGVfVZ4Osb9t1XVc8N\nHz4AHBhnBkmTczmv+tl2Y7/pLMkc8OmqOnSe5+4Bfq2q7tzktd50JrXEblr1s+1Guelsz7jCbCXJ\nzwFnNyuCNfPz8+e2u90u3W53vMEkjWRt1c+VlReu+mkZjFev16PX6+3oPRoZGSR5J/ATwBuq6pkL\nvNaRgdQSjgymx7QuR5Hhx+qD5Cbg/cDNFyoCSe1yua/62XZjHRkkuRPoAlcDp4FjwO3APuBPh4c9\nUFW3bvJ6RwZSy+yGVT/bzlVLJUlTO00kSZpyloEkyTKQJFkGkiQsA0kSloEkCctAkoRlIEnCMpAk\nYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJjLkMkiwkOZ1kad2+q5Lcm+SL\nSX4nyZXjzCBJ2tq4RwbHgRs37LsNuK+qXgXcD3xwzBkkSVsYaxlU1WeBr2/Y/Sbgo8PtjwL/eJwZ\nJElba+Kawcuq6jRAVT0FvKyBDJKkdfY0HQCoCz05Pz9/brvb7dLtdsccR5Lapdfr0ev1dvQeqbrg\n38U7lmQO+HRVHRo+fgzoVtXpJK8ATlbVd23y2hp3PknabZJQVdnOayYxTZThx5p7gHcOt98BfGoC\nGSRJFzDWkUGSO4EucDVwGjgGfBK4G/hbwDLw5qr6s01e78hAkrZplJHB2KeJdsIykKTtm9ZpIknS\nlLMMJEmWgSTJMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKwDCRJWAaSJCwD\nSRKWgSQJy0CShGUgSQL2XMxBSQ4DrwcK+L2qenCnJ07yL4BbgOeAU8C7qurMTt9XkrR9W44Mkvwr\n4KPA1cBLgeNJ/uVOTprkbwI/DRyuqkOsltJbdvKe0m4xGAxYXFxkMBg0HUWXkYuZJvqnwGuq6lhV\nHQNuAN5+Cc79DcA3J9kDXAF85RK8p9RqJ07cxdzcQY4efTdzcwc5ceKupiPpMnExZfAV4JvWPf5G\n4I93ctKq+grw74Enhu/1Z1V1307eU2q7wWDALbfcysrKSZ5++gusrJzklltudYSgibiYawZPA48k\n+Qyr1wyOAp9L8ksAVfXe7Z40ybcCbwLmhu//8SRvq6o7Nx47Pz9/brvb7dLtdrd7OqkV+v0++/Z1\nWFk5NNxziL175+j3+8zOzjaaTdOt1+vR6/V29B6pqgsfkLzjQs9X1Ue3fdLkR4Abq+onho/fDry2\nqn5qw3G1VT5ptxgMBszNHWRl5SRwCFhiZuYIy8uPWwbaliRUVbbzmi1HBqP8ZX8RngBuSPJNwDPA\nG4HFMZxHao3Z2VkWFu7glluOsHfvHGfPLrOwcIdFoInYdGSQ5GNV9eYkp1idHnqe4U8BjX7i5Bir\nP0F0FngI+PGqOrvhGEcGuuwMBgP6/T6dTsci0EhGGRlcqAz+RlV9NcnHgPevfwr4hap68+hRLzKc\nZSBJ23ZJp4mq6qvDze+squUNJzo4Qj5J0pTatAyS/CRwK3BNkqV1T70Y+L1xB5MkTc6FpomuBK4C\n/i1w27qn/qKq/u8EsjlNJEkjuKTXDKaBZSBJ2zdKGbhqqSTJMpAkWQbahVz1U9o+y0C7iqt+SqPx\nArJ2Ddf2kVZ5AVmXtbVVP1eLANav+inpwiwD7RqdToczZ/rA2j2SS5w9u0yn02kulNQSloF2jbVV\nP2dmjrB//2FmZo646qd0kbxmoF3HVT91ufMOZEmSF5AlSaOxDCRJloEkyTKQJGEZSJJosAySXJnk\n7iSPJXkkyWubyiJJl7tNf+3lBHwI+M2q+tEke4ArGswiSZe1Ru4zSLIfeKiqvmOL47zPQJK2qU33\nGXw78LUkx5M8mOTDSWYayiJJl72mpon2AIeB91TV55P8B+A24NjGA+fn589td7tdut3uhCJKUjv0\nej16vd6O3qOpaaKXA/+rqq4ZPn498IGq+uENxzlNJEnb1Jppoqo6DTyZ5NrhrjcCjzaRRZLU4EJ1\nSb4b+BVgL/CHwLuq6ukNxzgykKRtctVSSVJ7pokkSdPFMpAkWQaSJMtAkoRlIEnCMpAkYRloE4PB\ngMXFRQaDQdNRJE2AZaAXOHHiLubmDnL06LuZmzvIiRN3NR1J0ph505meZzAYMDd3kJWVk8AhYImZ\nmSMsLz/O7Oxs0/EkXQRvOtOO9ft99u3rsFoEAIfYu3eOfr/fXChJY2cZ6Hk6nQ5nzvSBpeGeJc6e\nXabT6TQXStLYWQZ6ntnZWRYW7mBm5gj79x9mZuYICwt3OEUk7XJeM9B5DQYD+v0+nU7HIpBaxlVL\nJUleQJYkjcYykCRZBpIky0CShGUgSaLhMkjyoiQPJrmnyRySdLlremTwPuDRhjNI2+KKrtqNGiuD\nJAeAHwR+pakM0na5oqt2q8ZuOktyN/CvgSuBn62qm89zjDedaWq4oqvaYpSbzvaMK8yFJPkh4HRV\nPZykC2waen5+/tx2t9ul2+2OO550Xmsruq6svHBFV8tATer1evR6vR29RyMjgyT/BvhnwLPADPBi\n4BNV9WMbjnNkoKnhyEBt0ZrlKKrq9qp6ZVVdA7wFuH9jEUjTxhVdtZs1vlBdku/HawZqEVd01bRz\n1VJJUnumiSRJ08UykCRZBpIky0CShGUgScIykCRhGaghrvwpTRfLQBPnyp/S9PGmM02U6/tI4+dN\nZ5p6ayt/rhYBrF/5U1JzLANNVKfT4cyZPrA03LPE2bPLdDqd5kJJsgw0Wa78KU0nrxmoEa78KY2P\nq5ZKkryALEkajWUgSbIMJEmWgSQJy0CSRENlkORAkvuTPJLkVJL3NpFDkrSqqZHBs8DPVNV1wPcB\n70lysKEsYzHuVTld9VPSpdRIGVTVU1X18HD7L4HHgG9rIss4jHtVTlf9lHSpNX7TWZIO0AP+zrAY\n1j/XupvOxr0qp6t+StrKKDed7RlXmIuR5FuAjwPv21gEa+bn589td7tdut3uRLKNam1VzpWVF67K\neSn+sh73+0tqn16vR6/X29F7NDYySLIH+A3gt6rqQ5sc48hgwu8vqf3athzFR4BHNyuCthr3qpyu\n+ilpHBoZGSR5HfA/gFNADT9ur6rf3nBc60YGa8a9KqerfkrajKuWSpJaN00kSZoSloEkyTKQJFkG\nkiQsA0kSloEkCctAkoRlIEnCMpAkYRlIkrAMJElYBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJJo\nsAyS3JTk8SR/kOQDTeWQJDVUBkleBPwycCNwHfDWJAebyDJOvV6v6Qg70ub8bc4O5m9a2/OPoqmR\nwfXAl6pquarOAr8GvKmhLGPT9m+oNudvc3Ywf9Pann8UTZXBtwFPrnv8R8N9kqQGeAFZkkSqavIn\nTW4A5qvqpuHj24Cqqn+34bjJh5OkXaCqsp3jmyqDbwC+CLwR+CrwOeCtVfXYxMNIktjTxEmr6q+T\n/BRwL6tTVQsWgSQ1p5GRgSRpukzlBeQ23pCWZCHJ6SRL6/ZdleTeJF9M8jtJrmwy42aSHEhyf5JH\nkpxK8t7h/rbk/8Yk/zvJQ8P8x4b7W5EfVu+9SfJgknuGj1uTHSBJP8nvD/8ffG64rxWfQ5Irk9yd\n5LHhn4HXtij7tcOv+YPD/z6d5L2j5J+6MmjxDWnHWc283m3AfVX1KuB+4IMTT3VxngV+pqquA74P\neM/wa96K/FX1DHCkqr4X+B7gB5JcT0vyD70PeHTd4zZlB3gO6FbV91bV9cN9bfkcPgT8ZlV9F/Dd\nwOO0JHtV/cHwa34Y+LvAXwH/nVHyV9VUfQA3AL+17vFtwAeaznWR2eeApXWPHwdePtx+BfB40xkv\n8vP4JPAP25gfuAL4PPCatuQHDgCfAbrAPW383gG+DFy9Yd/Ufw7AfuD/nGf/1Gc/T+Z/BPzuqPmn\nbmTA7roh7WVVdRqgqp4CXtZwni0l6bD6r+sHWP1makX+4TTLQ8BTwGeqapH25P9F4P3A+gt4bcm+\npoDPJFlM8uPDfW34HL4d+FqS48Oplg8nuYJ2ZN/onwB3Dre3nX8ay2A3m+qr9Um+Bfg48L6q+kte\nmHdq81fVc7U6TXQAuD7JdbQgf5IfAk5X1cPAhX4ufOqyb/C6Wp2q+EFWpxn/Pi34+rP6E5WHgf80\nzP9XrM5GtCH7OUn2AjcDdw93bTv/NJbBHwOvXPf4wHBfG51O8nKAJK8A/qThPJtKsofVIvjVqvrU\ncHdr8q+pqj8HesBNtCP/64Cbk/whcAJ4Q5JfBZ5qQfZzquqrw/8OWJ1mvJ52fP3/CHiyqj4/fPzr\nrJZDG7Kv9wPAF6rqa8PH284/jWWwCHxnkrkk+4C3APc0nOlihef/6+4e4J3D7XcAn9r4ginyEeDR\nqvrQun2tyJ/kpWs/LZFkBjgKPEYL8lfV7VX1yqq6htXv9fur6u3Ap5ny7GuSXDEcVZLkm1mduz5F\nO77+p4Enk1w73PVG4BFakH2Dt7L6j4k128/f9EWPTS6E3MTqHcpfAm5rOs9FZr4T+ArwDPAE8C7g\nKuC+4edyL/CtTefcJPvrgL8GHgYeAh4c/j94SUvyv3qY+WFgCfi54f5W5F/3eXw///8Ccmuyszrv\nvva9c2rtz2xbPgdWf4Jocfg5fAK4si3Zh/mvAAbAi9ft23Z+bzqTJE3lNJEkacIsA0mSZSBJsgwk\nSVgGkiQsA0kSloF0QcObH0+dZ/+H11bTTfLlJC8Zbv/FpDNKl0Ijv+lMapkX3IxTVf98k+e9cUet\n5MhA2treJP81yaNJPpZkJsnJJIeHz2/rF49L08gykLb2KuCXq+pvA38O3IojAO0yloG0tSeq6oHh\n9n8DXt9kGGkcLANpa61a214ahWUgbW0uyWuH228DfpfNrxN4/UCtZBlIW3uc1d/e9Siryxv/Zzb/\nCSJHDWoll7CWJDkykCRZBpIkLANJEpaBJAnLQJKEZSBJwjKQJGEZSJKA/wf9pS+zct851AAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d2f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(tip_data.???, tip_data.???)\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spark Dataframe\n",
    "\n",
    "Let's now take our pandas dataframe and make a spark dataframe out of it.\n",
    "\n",
    "**=>TODO: load the pandas dataframe tip_data into a spark dataframe spark_tips **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_tips = spark.createDataFrame(???)\n",
    "spark_tips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a Features column with the Vector\n",
    "\n",
    "We need to create a feature vector here.  You might wonder why we need a \"vector\" after all there's only one variable: \"bill.\"  Nonetheless, we still need the dataframe to have a column called \"features\" of type Vector Double).\n",
    "\n",
    "Luckily, there's a handy class called VectorAssembler that does this for us.\n",
    "\n",
    "**=>TODO: build a vector from the input column \"bill\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[???], outputCol=\"features\")\n",
    "featureVector = assembler.transform(spark_tips)\n",
    "\n",
    "featureVector.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Label column with the Vector\n",
    "\n",
    "We need a way to have the outcome variable.  In this case, we are trying to predict the tip from the bill.  So, we need to signal to Spark MLlib which one is the outcome variable.  In this case, we'll create a new variable called label.  Since we already have that, we can just rename the \"tip\" column as the \"label\".\n",
    "\n",
    "**=>TODO: rename variable \"tip\" as \"label\" **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureVector = featureVector.withColumnRenamed(\"tip\", \"???\")\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Run Linear Regression in Spark\n",
    "\n",
    "Let's run our linear regression.  To do this we need to run call the LinearRegression Classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(featureVector)\n",
    "\n",
    "\n",
    "intercept = lrModel.intercept  # This is the intercept\n",
    "slope = lrModel.coefficients[0] #This is the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plot the fit line (abline)\n",
    "\n",
    "**=>TODO: Do a scatterplot of bill versus tip **\n",
    "**=>TODO: rename variable \"tip\" as \"label\" **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values in the best fit line\n",
    "abline_values = [slope * i + intercept for i in tip_data.bill]\n",
    "\n",
    "# Plot the best fit line over the actual values\n",
    "plt.scatter(tip_data.???, tip_data.???)\n",
    "plt.plot(tip_data.bill, abline_values, 'b')\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Fit Line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print out the Outputs\n",
    "\n",
    "Here is a sample output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Plot the residuals\n",
    "\n",
    "Residuals are the error, or difference between the model predicted and model actual.  We'd like these to be as small as possible, with residuals roughly balanced.   We don't want a model that consistently predicts values too high or too low.\n",
    "\n",
    "**=>TODO: do a plot of the bill (x-value) versus residuals (y-value) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = lrModel.summary.residuals.toPandas()\n",
    "plt.scatter(tip_data.???, residuals['residuals'])\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # horizon\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Identify Coefficients\n",
    "\n",
    "### Intercept and Slope\n",
    "We can see them from output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "source": [
    "Coefficients:\n",
    "            Estimate \n",
    "(Intercept) -0.8217112049846651\n",
    "bill        0.226334605857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Slope** (of line) : ** 0.226334605857**\n",
    "- **Intercept** (where line meets Y-axis) : **-0.8217112049846651**  (below zero line)\n",
    "\n",
    "We can also get these programatically.  \n",
    "If `tip = a * amount + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "a = lrModel.coefficients[0]\n",
    "b = lrModel.intercept\n",
    "\n",
    "intercept = lrModel.intercept\n",
    "slope = lrModel.coefficients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient (r)\n",
    "Correlation Coefficient (r) denotes how strongly X & Y are inter-related.  \n",
    "\n",
    "Spark ML can create a correlation Matrix which shows how all the variables in the dataset are correlated, if at all.  Naturally, every variable will have a correlation of 1 when compared to itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"bill\", \"tip\"], outputCol=\"features\")\n",
    "correlationVector = assembler.transform(spark_tips)\n",
    "\n",
    "r1 = Correlation.corr(correlationVector, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==>  Question : Does bill amount influence tip amount? (are they strongly linked?) **\n",
    "\n",
    "\n",
    "## Coefficient of Determination (r&sup2;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coefficient.of.determination (r^2) \n",
    "rsquared = lrModel.summary.r2\n",
    "# 0.9067141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==>  Question : What does r&sup2; tells us? **\n",
    "\n",
    "## Step 11: Calculate tip for $100 bill\n",
    "\n",
    "**=>TODO: perform the calculation for tip_for_100 using a,b coefficients **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tip_for_100 = ???   \n",
    "print(tip_for_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Add a estimated_tip column to pandas and spark dataframe\n",
    "\n",
    "**=>TODO: create a new pandas column called est_tip **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tip_data['est_tip'] =  ???\n",
    "\n",
    "tip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add estimated tip column to pyspark dataframe\n",
    "\n",
    "This is a bit tricky. We need to use the sql expr function to make this work.\n",
    "\n",
    "The formula: (bill * a) + b\n",
    "\n",
    "**=>TODO: take formula and create new spark column called est_tip **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "formula = \"(bill * \" + str(a) + \") + \" + str(b)\n",
    "\n",
    "print(formula)\n",
    "## Step 6: Add estimated tip column to spark dataframe\n",
    "spark_tips_with_est = spark_tips.withColumn(\"est_tip\", expr(???))\n",
    "spark_tips_with_est.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Perform a prediction\n",
    "\n",
    "\n",
    "Let's try to run a prediction on some data: $45.00, $55.00, and $65.00 \n",
    "\n",
    "\n",
    "**=>TODO: use model to transform dataframe with feature vectors to make predictions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_pd = pd.DataFrame({'bill' : [45., 55., 65.],\n",
    "             })\n",
    "test_data_spark =  spark.createDataFrame(test_data_pd)\n",
    "test_assembler = VectorAssembler(inputCols=[\"bill\"], outputCol=\"features\")\n",
    "test_features = test_assembler.transform(test_data_spark)\n",
    "\n",
    "\n",
    "test_predictions = lrModel.transform(???)\n",
    "\n",
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Complete Code\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "# tip = a * amount + b\n",
    "\n",
    "# Make a Pandas dataframe\n",
    "tip_data = pd.DataFrame({'bill' : [50.00, 30.00, 60.00, 40.00, 65.00, 20.00, 10.00, 15.00, 25.00, 35.00],\n",
    "              'tip' : [12.00, 7.00, 13.00, 8.00, 15.00, 5.00, 2.00, 2.00, 3.00, 4.00]\n",
    "             })\n",
    "                       \n",
    "print(tip_data)\n",
    "\n",
    "#Plot the Tip Data\n",
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a Spark Dataframe\n",
    "spark_tips = spark.createDataFrame(tip_data)\n",
    "spark_tips.show()\n",
    "\n",
    "# Create a Features Columns\n",
    "assembler = VectorAssembler(inputCols=[\"bill\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(spark_tips)\n",
    "\n",
    "featureVector.show()\n",
    "\n",
    "# Create the label column\n",
    "featureVector = featureVector.withColumnRenamed(\"tip\", \"label\")\n",
    "featureVector.show()\n",
    "\n",
    "# Run Linear Regression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(featureVector)\n",
    "\n",
    "\n",
    "intercept = lrModel.intercept  # This is the intercept\n",
    "slope = lrModel.coefficients[0] #This is the slope\n",
    "\n",
    "# Plot the best fit line over the actual values\n",
    "abline_values = [slope * i + intercept for i in tip_data.bill]\n",
    "plt.scatter(tip_data.bill, tip_data.tip)\n",
    "plt.plot(tip_data.bill, abline_values, 'b')\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Fit Line\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "#Plot Residuals\n",
    "residuals = lrModel.summary.residuals.toPandas()\n",
    "plt.scatter(tip_data.bill, residuals['residuals'])\n",
    "plt.axhline(y=0, color='r', linestyle='-')  # horizon\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('bill')\n",
    "plt.title(\"Residuals\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Tip for $100 bill\n",
    "tip_for_100 = a * 100 + b   \n",
    "print(tip_for_100)\n",
    "\n",
    "\n",
    "# Add est_tip to dataframes\n",
    "tip_data['est_tip'] = tip_data.bill * a + b  #Pandas\n",
    "tip_data\n",
    "\n",
    "formula = \"(bill * \" + str(a) + \") + \" + str(b)  #Spark\n",
    "spark_tips_with_est = spark_tips.withColumn(\"est_tip\", expr(formula))\n",
    "spark_tips_with_est.show()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

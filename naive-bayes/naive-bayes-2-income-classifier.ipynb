{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Income Classifier\n",
    "We are going to use data from US Census to predict income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "dataset = spark.read.format(\"csv\").\\\n",
    "          option('header','true').\\\n",
    "          option('inferSchema', 'true').\\\n",
    "          load(\"/data/census-income/income-cleaned.csv\")\n",
    "\n",
    "t2 = time.perf_counter() \n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(dataset.count(), (t2-t1)*1000))\n",
    "\n",
    "dataset.printSchema()\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Analytics\n",
    "0 is income <=50k\n",
    "1 is income > 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupBy('income-over-50k').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Create Feature Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Index all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## TODO : add numerical columns.\n",
    "##   examine the data and add numerical columns\n",
    "##   Hint : start with 'age', 'education-num', 'hours-per-week'\n",
    "numeric_columns = ['age', '???', '???' ]\n",
    "\n",
    "categorical_columns = ['employment', 'education', 'marital-status', 'occupation', 'race', 'sex', 'native-country']\n",
    "categorical_index = ['employment_index', 'education_index', 'marital-status_index', 'occupation_index', \n",
    "                     \"race_index\", 'sex_index', 'native-country_index']\n",
    "input_cols = numeric_columns + categorical_index\n",
    "\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\").fit(dataset)\\\n",
    "            for column in categorical_columns ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "dataset_indexed = pipeline.fit(dataset).transform(dataset)\n",
    "t2 = time.perf_counter()\n",
    "dataset_indexed.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"indexed {:,} records in {:,.2f} ms\".format(dataset.count(), (t2-t1)*1000))\n",
    "\n",
    "## Save as CSV for easy viewing in Excel\n",
    "dataset_indexed.write.\\\n",
    "        option('header', 'true').\\\n",
    "        mode('overwrite').\\\n",
    "        csv('out-indexed')\n",
    "print(\"Saved indexed vector to 'out-indexed' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Create feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : set inputCols = input_cols\n",
    "assembler = VectorAssembler(inputCols=???, outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset_indexed)\n",
    "featureVector = featureVector.withColumn(\"label\",featureVector[\"income-over-50k\"])\n",
    "featureVector.select(['features', 'label']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Train / Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = featureVector.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "print(\"training set count : \", train.count())\n",
    "print(\"testing set count : \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Create Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "## TODO : Create a NB model with these parameters\n",
    "##     smoothing = 1.0\n",
    "##     modelType = 'multinomial'\n",
    "nb = NaiveBayes(smoothing=???, modelType='???')\n",
    "\n",
    "# train the model\n",
    "t1 = time.perf_counter()\n",
    "model = nb.fit(train)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"trained on {:,} records  in {:,.2f} ms\".\\\n",
    "      format(train.count(), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Test Data\n",
    "Let's call .transform on our model to do make predictions on our test data. The output should be contained in the \"prediction\" column, while the correct label will be there in the \"label\" column.\n",
    "\n",
    "We will be able to evaluate our results by comparing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.select(['label', 'prediction']).\\\n",
    "            sampleBy(col='prediction', fractions={0: 0.5, 1: 0.5}).\\\n",
    "            show()\n",
    "\n",
    "\n",
    "predictions.groupBy('prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('label').\\\n",
    "            pivot('prediction', [0,1]).\\\n",
    "            count().na.fill(0).\\\n",
    "            orderBy('label').\\\n",
    "            show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Discuss Model Accuracy\n",
    "Discuss how to improve model accuracy?  Here are some points to consider.\n",
    "\n",
    "- can you add any more input variables?\n",
    "- why is the model bad at predicting >50k income category?  Check the original data for skew?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

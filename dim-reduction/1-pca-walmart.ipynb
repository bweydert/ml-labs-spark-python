{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA in Spark ML\n",
    "\n",
    "Let's try our hands at dimensionality reduction in Spark MLLib.  We are going to look at the Walmart Dataset. Remember how many dimensions we had in that one? (70, to be exact). Perhaps we can get a lower-dimensional representation of that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/walmart-triptype/train-transformed.csv.gz\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Vectors\n",
    "\n",
    "Let's load the data and create vectors out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = dataset.columns\n",
    "columns.remove('VisitNumber') #We don't care about visit number as a feature.\n",
    "columns.remove('TripType') #Triptype is what we're predicting!\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the vector.\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print some sample rows.\n",
    "for row in featureVector.select('features').take(10):\n",
    "    print(\"Vector: %s\\n\" % (str(row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the output. These are Sparse (not dense) Vectors.  That's because we our data IS sparse, we have relatively few of the variables at any given time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build a Correlation Matrix\n",
    "\n",
    "We're going to build a correlation matrix.  This will have all features as rows and columns, showing what is the correlation between variables.  Naturally, every feature will be perfectly correlated to itself, so we expect to see a diagonal of 1's.  Correlations in the upper right and lower left should also be mirror images of each other.\n",
    "\n",
    "Perfectly uncorrelated features (orthogonal) would have the identity matrix as its correlation matrix.  Part of our goal in PCA is to create orthogonal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Checking the correlation matrix of the data.\n",
    "\n",
    "r1 = Correlation.corr(featureVector, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are some correlated dimensions in the original dataset.  We can identify this by the nonzero values in the correlation matrix.  Naturally, dimensions are always related to themselves with a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scale (Normalize) the data \n",
    "\n",
    "We need to scale our features so we do not have one dimension dominate. Why does this matter? Since some dimensions are scaled differently than others, those dimensions will be unfairly weighted in our analysis. We want to avoid this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(featureVector)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "ScaledFeatures = scalerModel.transform(featureVector)\n",
    "ScaledFeatures.select('features', 'scaledFeatures').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running PCA\n",
    "\n",
    "Now we will run PCA to reduce and uncorrelate dimensions.  \n",
    "\n",
    "**Try with five dimensions to start with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vars = ???\n",
    "\n",
    "pca = PCA(k=5, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(ScaledFeatures)\n",
    "pcaFeatures = model.transform(ScaledFeatures).select(\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the transformed dataset.  let's look at a distribution of our transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = model.explainedVariance.toArray()\n",
    "print(S)\n",
    "print(\"Cumulative Explained Variance: \" + str(np.cumsum(S)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a Scree plot\n",
    "\n",
    "This will show us the cumulative explained variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "S = model.explainedVariance.toArray()\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sing_vals = np.arange(num_vars) + 1\n",
    "plt.plot(np.arange(num_vars) + 1, np.cumsum(S), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "\n",
    "leg = plt.legend(['Explained Variance'], loc='best', borderpad=0.3, \n",
    "                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
    "                 markerscale=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?  Is the cumulative explained variance enough to represent our data in fewer dimensions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Re-Running the Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Checking the correlation matrix of the data.\n",
    "\n",
    "r1 = Correlation.corr(pcaFeatures, \"pcaFeatures\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the very small, close to zero correlations in the matrix.  The 5 dimensions are for all practical purposes independent and orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Running PCA with more dimensions to get Explained Variance higher\n",
    "\n",
    "Try to find at least 80% of explained variance.\n",
    "\n",
    "Can we use the elbow method here?  Why (or why not)?\n",
    "\n",
    "What does this say about the relative correlation of the dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_vars = ??? # Enter number of dimensions for explained Variance her.\n",
    "pca = PCA(k=num_vars, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(ScaledFeatures)\n",
    "pcaFeatures = model.transform(ScaledFeatures).select(\"pcaFeatures\")\n",
    "\n",
    "\n",
    "S = model.explainedVariance.toArray()\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sing_vals = np.arange(num_vars) + 1\n",
    "plt.plot(np.arange(num_vars) + 1, np.cumsum(S), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "\n",
    "leg = plt.legend(['Explained Variance'], loc='best', borderpad=0.3, \n",
    "                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
    "                 markerscale=0.4)\n",
    "\n",
    "print(\"Cumulative Explained Variance = \" + str(np.cumsum(S)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "What are your conclusions?  Were we able to reduce dimensions in this dataset without losing much of the \"signal\" of the data?\n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

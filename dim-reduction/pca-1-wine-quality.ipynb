{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on wine quality data\n",
    "We like this data, because it is all numeric data.\n",
    "It looks like this\n",
    "\n",
    "```\n",
    "\"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
    "7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\n",
    "7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\n",
    "7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.8;5\n",
    "```\n",
    "\n",
    "Check data in *datasets/wine-quality*\n",
    "- winequality-red.csv\n",
    "- winequality-white.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "## TODO : try red and white wine data\n",
    "data_file= '/data/wine-quality/winequality-red.csv'\n",
    "#data_file= '/data/wine-quality/winequality-white.csv'\n",
    "column_to_remove = 'quality'\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "data = spark.read.\\\n",
    "          option('header', 'true').\\\n",
    "          option('inferSchema', 'true').\\\n",
    "          option('delimiter', ';').\\\n",
    "          csv(data_file)\n",
    "t2 = time.perf_counter()\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(data.count(), (t2-t1)*1000))\n",
    "\n",
    "data_clean = data.na.drop()\n",
    "print(\"raw data count {},  cleaned data count {}\".format(data.count(), data_clean.count()))\n",
    "\n",
    "data_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove columns we don't need\n",
    "columns = data_clean.columns\n",
    "columns.remove(column_to_remove)  # this is the target , so remove\n",
    "data2 = data_clean.select(columns)\n",
    "\n",
    "print(\"original data columns  \", len(data2.columns))\n",
    "data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : use 'describe' to do basic data analytics\n",
    "data2.???().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "feature_vector = assembler.transform(data2)\n",
    "feature_vector.select('features').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Correlation Matrix of original data\n",
    "Do see any correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "## TODO : Identify components that are highly correlated\n",
    "\n",
    "corr1 = Correlation.corr(feature_vector, \"features\").head()\n",
    "pearson_corr = corr1[0]\n",
    "#print(\"Pearson correlation matrix:\\n\" + str(pearson_corr))\n",
    "\n",
    "## convert to numpy for pretty print\n",
    "print(\"Pearson correlation matrix\")\n",
    "np.set_printoptions(precision=2,  linewidth=200)\n",
    "pearson_corr_nparr = pearson_corr.toArray()\n",
    "print(pearson_corr_nparr)\n",
    "\n",
    "## convert to pandas for even prettier print :-)\n",
    "names = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\",\\\n",
    "         \"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\", \"sulphates\",\"alcohol\"]\n",
    "\n",
    "df = pd.DataFrame(pearson_corr_nparr, index=names, columns=names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Scale Data\n",
    "We need to scale data before PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "## TODO : create a scaler\n",
    "##   Hint : inputCol = 'features'\n",
    "##   Hint : outputCol  = 'scaledFeatures'\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"???\", outputCol=\"???\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scaler_model = scaler.fit(feature_vector)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "fv_scaled = scaler_model.transform(feature_vector)\n",
    "fv_scaled.select('features', 'scaledFeatures').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "## TODO : create 5 Principal Components\n",
    "num_pc = ???\n",
    "\n",
    "pca = PCA(k=num_pc, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(fv_scaled)\n",
    "pca_features = model.transform(fv_scaled).select(\"pcaFeatures\")\n",
    "pca_features.select('pcaFeatures').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Correlation Matrix for Principal Components\n",
    "These should be very small (close to zero!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "## correlation matrix for PC\n",
    "## should be very close to zero\n",
    "corr_pc = Correlation.corr(pca_features, \"pcaFeatures\").head()[0]\n",
    "corr_pc_nparr = corr_pc.toArray()\n",
    "\n",
    "print (\"Correlation Matrtix for Principal Components\")\n",
    "np.set_printoptions(precision=2, suppress=False)\n",
    "print(corr_pc_nparr)\n",
    "print()\n",
    "\n",
    "print (\"Correlation Matrtix for Principal Components\")\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(corr_pc_nparr)\n",
    "\n",
    "## TODO : Inspect at correlations for PC\n",
    "##      Are they highly correlated?  Not? can you explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Calculate PC Variance\n",
    "\n",
    "We started with 5 PCs.  \n",
    "How much coverage (variance) are we getting?\n",
    "\n",
    "Play with **num_pc** in Step-6 to get 90% coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variance\n",
    "variance = model.explainedVariance.toArray()\n",
    "print(variance)\n",
    "print (\"Original data had {} features,  principal components {}\".format(len(data2.columns), num_pc))\n",
    "print(\"Cumulative Explained Variance: \" + str(np.cumsum(variance)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Screeplot\n",
    "Screeplot goes from 0.0  to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = model.explainedVariance.toArray()\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sing_vals = np.arange(num_pc) + 1\n",
    "plt.plot(np.arange(num_pc) + 1, np.cumsum(variance), 'ro-', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "\n",
    "\n",
    "leg = plt.legend(['Explained Variance'], loc='best', borderpad=0.3, \n",
    "                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
    "                 markerscale=0.4)\n",
    "\n",
    "## TODO : Explain the screeplot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

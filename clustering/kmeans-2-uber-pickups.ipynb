{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering : K-Means : Uber Pickups\n",
    "\n",
    "This is data of Uber pickups in New York City.  \n",
    "The data is from this [kaggle competition](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city).\n",
    "\n",
    "Sample data looks like this\n",
    "```\n",
    "\"Date_Time\",\"Lat\",\"Lon\",\"Base\"\n",
    "\"4/1/2014 0:11:00\",40.769,-73.9549,\"B02512\"\n",
    "\"4/1/2014 0:17:00\",40.7267,-74.0345,\"B02512\"\n",
    "\"4/1/2014 0:21:00\",40.7316,-73.9873,\"B02512\"\n",
    "\"4/1/2014 0:28:00\",40.7588,-73.9776,\"B02512\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "We will also specify schema to reduce loading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file to read\n",
    "\n",
    "## sample file with 10,000 records\n",
    "data_file=\"/data/uber-nyc/uber-sample-10k.csv\"\n",
    "\n",
    "## larger file with about 500k records\n",
    "# data_file = \"/data/uber-nyc/uber-raw-data-apr14.csv.gz\"\n",
    "\n",
    "## all data\n",
    "# data_file = \"/data/uber-nyc/*csv.gz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, StructField, StructType\n",
    "\n",
    "\n",
    "pickup_time_field = StructField(\"pickup_time\", StringType(), True)\n",
    "lat_field = StructField(\"Lat\", FloatType(), True)\n",
    "lon_field = StructField(\"Lon\", FloatType(), True)\n",
    "base_field = StructField(\"Base\", StringType(), True)\n",
    "\n",
    "schema = StructType([pickup_time_field, lat_field, lon_field, base_field])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "uber_pickups = spark.read.option(\"header\", \"true\").schema(schema).csv(data_file)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "records_count_total = uber_pickups.count()\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(records_count_total, (t2-t1)*1000))\n",
    "uber_pickups.printSchema()\n",
    "uber_pickups.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleanup data\n",
    "make sure our data is clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uber_pickups_clean = uber_pickups.na.drop(subset=['Lat', 'Lon'])\n",
    "records_count_clean = uber_pickups_clean.count()\n",
    "\n",
    "print (\"cleaned records {:,},  dropped {:,}\".format(records_count_clean,  (records_count_total - records_count_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : create a feature vectors using 'Lat'  and 'Lon'  attributes\n",
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(uber_pickups_clean)\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running Kmeans\n",
    "\n",
    "Now it's time to run kmeans on the resultant dataframe.  We don't know what value of k to use, so let's just start with k=4.  This means we will cluster into four groups.\n",
    "\n",
    "We will fit a model to the data, and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "## TODO : start with 4 clusters\n",
    "num_clusters = ???\n",
    "kmeans = KMeans().setK(num_clusters).setSeed(1)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "## TODO : fit (featureVector)\n",
    "model = kmeans.fit(???)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "wssse = model.computeCost(featureVector)\n",
    "\n",
    "\n",
    "print(\"Kmeans : {} clusters computed in {:,.2f} ms\".format( num_clusters,  ((t2-t1)*1000)))\n",
    "print (\"num_clusters = {},  WSSSE = {:,}\".format(num_clusters, wssse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Let's find the best K - Hyperparameter tuning\n",
    "\n",
    "Let's try iterating and plotting over values of k, so we can practice using the elbow method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kvals = []\n",
    "wssses = []\n",
    "\n",
    "## TODO : loop over k values from 2 to 10\n",
    "for k in range(???, ????):\n",
    "    kmeans = KMeans().setK(k).setSeed(1)\n",
    "    t1 = time.perf_counter()\n",
    "    model = kmeans.fit(featureVector)\n",
    "    t2 = time.perf_counter()\n",
    "    wssse = model.computeCost(featureVector)\n",
    "    print (\"k={},  wssse={},  time took {:,.2f} ms\".format(k,wssse, ((t2-t1)*1000)))\n",
    "    kvals.append(k)\n",
    "    wssses.append(wssse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'k': kvals, 'wssse':wssses})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.plot(kvals, wssses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Let's run K-Means with the best K we have choosen\n",
    "From the graph above, choose a good K value.  We wwill use that below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO : pick a K value\n",
    "num_clusters = ???\n",
    "kmeans = KMeans().setK(num_clusters).setSeed(1)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "model = kmeans.fit(featureVector)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "wssse = model.computeCost(featureVector)\n",
    "\n",
    "\n",
    "print(\"Kmeans : {} clusters computed in {:,.2f} ms\".format( num_clusters,  ((t2-t1)*1000)))\n",
    "print (\"num_clusters = {},  WSSSE = {:,}\".format(num_clusters, wssse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "predicted = model.transform(featureVector)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"{:,} records clustered in {:,.2f} ms\".format(predicted.count(), ((t2-t1)*1000) ))\n",
    "\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Print Cluster Center and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_count = predicted.groupby(\"prediction\").count().orderBy(\"prediction\")\n",
    "cluster_count.show()\n",
    "index = 0\n",
    "for c in model.clusterCenters():\n",
    "    print(index, c)\n",
    "    index = index+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Ploting time!\n",
    "We are going to plot the results now.  \n",
    "Since we are dealing with GEO co-ordinates, let's use Google Maps!  \n",
    "\n",
    "Go to the following URL :  \n",
    "[https://jsfiddle.net/sujee/omypetfu/](https://jsfiddle.net/sujee/omypetfu/)\n",
    "\n",
    "- Run the code cell below\n",
    "- copy paste the output into Javascript section of the JSFiddle Editor (lower left)\n",
    "- and click 'Run'  (top nav bar)\n",
    "- Click on 'tidy' (top nav bar)  to cleanup code\n",
    "\n",
    "See the following image \n",
    "\n",
    "<img src=\"../assets/images/kmeans_uber_trips_map.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n",
    "You will be rewarded with a beautiful map of clusters on Google Maps\n",
    "\n",
    "<img src=\"../assets/images/Kmeans_uber_trips.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n",
    "Optional\n",
    "- You can 'fork' the snippet and keep tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### generate Javascript\n",
    "s1 = \"var clusters = {\"\n",
    "\n",
    "s2 = \"\"\n",
    "\n",
    "prediction_count = predicted.groupby(\"prediction\").count().orderBy(\"prediction\").select(\"count\").collect()\n",
    "total_count = 0\n",
    "cluster_centers = model.clusterCenters()\n",
    "for i in range(0, num_clusters):\n",
    "    count = prediction_count[i][\"count\"]\n",
    "    lat = cluster_centers[i][0]\n",
    "    lng = cluster_centers[i][1]\n",
    "    total_count = total_count + count\n",
    "    if (i > 0):\n",
    "        s2 = s2 + \",\"\n",
    "    s2 = s2 + \" {}: {{ center: {{ lat: {}, lng: {} }}, count: {} }}\".\\\n",
    "        format(i, lat, lng, count)\n",
    "    #s2 = s2 + \"{}: {{  center: {{ }}, }}\".format(i)\n",
    "\n",
    "s3 = s1 + s2 + \"};\"\n",
    "\n",
    "s4 = \"\"\"\n",
    "function initMap() {\n",
    "  // Create the map.\n",
    "  var map = new google.maps.Map(document.getElementById('map'), {\n",
    "    zoom: 10,\n",
    "    center: {\n",
    "      lat: 40.77274573,\n",
    "      lng: -73.94\n",
    "    },\n",
    "    mapTypeId: 'roadmap'\n",
    "  });\n",
    "\n",
    "  // Construct the circle for each value in citymap.\n",
    "  // Note: We scale the area of the circle based on the population.\n",
    "  for (var cluster in clusters) {\n",
    "    // Add the circle for this city to the map.\n",
    "    var cityCircle = new google.maps.Circle({\n",
    "      strokeColor: '#FF0000',\n",
    "      strokeOpacity: 0.8,\n",
    "      strokeWeight: 2,\n",
    "      fillColor: '#FF0000',\n",
    "      fillOpacity: 0.35,\n",
    "      map: map,\n",
    "      center: clusters[cluster].center,\n",
    "\"\"\"\n",
    "\n",
    "s5 = \"radius: clusters[cluster].count / {} * 100 * 300 }});  }}}}\".format(total_count)\n",
    "\n",
    "# final\n",
    "s = s3 + s4 + s5\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 9: Let's analyze some more data\n",
    "\n",
    "- In Step-1 select the data_file to \n",
    "```\n",
    "data_file = \"/data/uber-nyc/uber-raw-data-apr14.csv.gz\"\n",
    "```\n",
    "- And select 'Cell --> Run All'  to execute all code blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Running the script\n",
    "\n",
    "**Use the dowload script**\n",
    "\n",
    "```bash\n",
    "cd   ~/data/uber-nyc\n",
    "./download-data.sh\n",
    "```\n",
    "\n",
    "This will download more data.\n",
    "\n",
    "As we run on larger dataset, the execution will take longer and Jupyter notebook might time out.  So let's run this in command line / script mode\n",
    "\n",
    "```bash\n",
    "\n",
    "$    cd   ~/ml-labs-spark-python/clustering\n",
    "\n",
    "$    time  ~/spark/bin/spark-submit    --master local[*]  kmeans-uber.py 2> logs\n",
    "\n",
    "```\n",
    "\n",
    "Watch the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
